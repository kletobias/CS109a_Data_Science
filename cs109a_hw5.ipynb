{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A: Homework 5\n",
    "# Logistic Regression and PCA \n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine\n",
    "\n",
    "---\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- Do not include your name(s) in the notebook if you are submitting as a group. \n",
    "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your partner's name (if you submit separately): Walter Thornton and Dwayne Kennemore\n",
    "\n",
    "Enrollment Status (109A, 121A, 209A, or E109A): E109A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import spline\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import linear_model, decomposition\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "#--------  visualize_prob\n",
    "# A function to visualize the probabilities predicted by a Logistic Regression model\n",
    "# Input: \n",
    "#      model (Logistic regression model)\n",
    "#      x (n x d array of predictors in training data)\n",
    "#      y (n x 1 array of response variable vals in training data: 0 or 1)\n",
    "#      ax (an axis object to generate the plot)\n",
    "\n",
    "def visualize_prob(model, x, y, ax):\n",
    "    # Use the model to predict probabilities for\n",
    "    # import numpy as np\n",
    "    y_pred = model.predict_proba(x)\n",
    "    \n",
    "    # Separate the predictions on the label 1 and label 0 points\n",
    "    ypos = y_pred[np.where(y==1)]\n",
    "    #ypos = y_pred[y==1]\n",
    "    yneg = y_pred[y==0]\n",
    "    \n",
    "    # Count the number of label 1 and label 0 points\n",
    "    npos = ypos.shape[0]\n",
    "    nneg = yneg.shape[0]\n",
    "    \n",
    "    # Plot the probabilities on a vertical line at x = 0, \n",
    "    # with the positive points in blue and negative points in red\n",
    "    pos_handle = ax.plot(np.zeros((npos,1)), ypos[:,1], 'bx', label ='ALL')\n",
    "    neg_handle = ax.plot(np.zeros((nneg,1)), yneg[:,1], 'rx', label = 'AML')\n",
    "    \n",
    "    # Line to mark prob 0.5\n",
    "    ax.axhline(y = 0.5, color = 'k', linestyle = '--')\n",
    "    \n",
    "    # Add y-label and legend, do not display x-axis, set y-axis limit\n",
    "    ax.set_ylabel('Probability of AML class')\n",
    "    ax.legend(loc = 'best')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.set_ylim([0,1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancer Classification from Gene Expressions\n",
    "\n",
    "In this homework assignment, we will build a classification model to distinguish between two related classes of cancer, acute lymphoblastic leukemia (ALL) and acute myeloid leukemia (AML), using gene expression measurements. The data set is provided in the file `dataset_hw5.csv`. Each row in this file corresponds to a tumor tissue sample from a patient with one of the two forms of Leukemia. The first column contains the cancer type, with 0 indicating the ALL class and 1 indicating the AML class. Columns 2-7130 contain expression levels of 7129 genes recorded from each tissue sample. \n",
    "\n",
    "In the following parts, we will use logistic regression to build a classification model for this data set. We will also use principal components analysis (PCA) to visualize the data and to reduce its dimensions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a): Data Exploration\n",
    "\n",
    "1. First step is to  split  the observations into an approximate 50-50 train-test split.  Below is some code to do this for you (we want to make sure everyone has the same splits).\n",
    "\n",
    "2. Take a peak at your training set: you should notice the severe differences in the measurements from one gene to the next (some are negative, some hover around zero, and some are well into the thousands).  To account for these differences in scale and variability, normalize each predictor to vary between 0 and 1.\n",
    "\n",
    "3. Notice that the results training set contains more predictors than observations. Do you foresee a problem in fitting a classification model to such a data set?\n",
    "\n",
    "4. A convenient tool to visualize the gene expression data is a heat map. Arrange the rows of the training set so that the 'AML' rows are grouped together and the 'ALL' rows are together. Generate a heat map of the data with expression values from the following genes: \n",
    "`D49818_at`, `M23161_at`, `hum_alu_at`, `AFFX-PheX-5_at`, `M15990_at`. By observing the heat map, comment on which of these genes are useful in discriminating between the two classes.\n",
    "\n",
    "5. We can also visualize this data set in two dimensions using PCA. Find the top two principal components for the gene expression data. Generate a scatter plot using these principal components, highlighting the AML and ALL points in different colors. How well do the top two principal components discriminate between the two classes?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe problem with fitting a classification model to a data set that is short is that when P > n the regression model\\nis not specified. The number of predictors has to be reduced by some technique (such as PCA analysis).\\n\\nWe made the heat map as asked, but we could not discern a meaningful relationship by looking at the gene expression levels\\nvisually. Perhaps if we used a different color scheme it would be more plain, but we decided we're going to just have to rely\\non the numbers.\\n\\nWe found the top two principal components explained around 28% of the variance.\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The problem with fitting a classification model to a data set that is short is that when P > n the regression model\n",
    "is not specified. The number of predictors has to be reduced by some technique (such as PCA analysis).\n",
    "\n",
    "We made the heat map as asked, but we could not discern a meaningful relationship by looking at the gene expression levels\n",
    "visually. Perhaps if we used a different color scheme it would be more plain, but we decided we're going to just have to rely\n",
    "on the numbers.\n",
    "\n",
    "We found the top two principal components explained around 28% of the variance.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wlt42\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import our data\n",
    "\n",
    "np.random.seed(9001)\n",
    "df = pd.read_csv('dataset_hw5.csv')\n",
    "msk = np.random.rand(len(df)) < 0.5\n",
    "data_train = df[msk]\n",
    "data_test = df[~msk]\n",
    "\n",
    "# Scale the data\n",
    "x = data_train.values \n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "data_train = pd.DataFrame(x_scaled, columns = data_train.columns)\n",
    "\n",
    "x = data_test.values \n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "data_test = pd.DataFrame(x_scaled, columns = data_test.columns)\n",
    "\n",
    "# For use later preserving column names for part D\n",
    "df_ = pd.DataFrame(data_train, columns = data_test.columns)\n",
    "df_ = df_.drop('Cancer_type', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD9CAYAAACsq4z3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e5b55dd048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make heatmap\n",
    "Cols = ['Cancer_type', 'D49818_at', 'M23161_at', 'hum_alu_at', 'AFFX-PheX-5_at', 'M15990_at']\n",
    "heatmap_df = data_train[Cols].copy()\n",
    "heatmap_df = heatmap_df.sort_values(by=\"Cancer_type\")\n",
    "heatmap_df\n",
    "\n",
    "plt.pcolor(heatmap_df)\n",
    "plt.yticks(np.arange(1, len(heatmap_df.index), 1), heatmap_df.index)\n",
    "plt.xticks(np.arange(1, len(heatmap_df.columns), 1), heatmap_df.columns)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Explained variance: [ 17.20614726  10.73253617]\n",
      "\n",
      "\n",
      "   target        pc1        pc2\n",
      "0     0.0   7.667012  -0.181414\n",
      "1     0.0  -8.705269  -3.125841\n",
      "2     0.0  21.341975   8.695756\n",
      "3     0.0  11.527633  23.669014\n",
      "4     0.0  -7.842507   7.473386\n"
     ]
    }
   ],
   "source": [
    "pca_input = data_train.drop('Cancer_type', axis=1)\n",
    "X_train_for_pca = np.array(pca_input)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "transformed_data = pca.fit_transform(X_train_for_pca)\n",
    "\n",
    "\n",
    "dfpca = pd.DataFrame({\"target\" : data_train['Cancer_type']})\n",
    "for i in range(pca.explained_variance_ratio_.shape[0]):\n",
    "    dfpca[\"pc%i\" % (i+1)] = transformed_data[:,i]\n",
    "\n",
    "\n",
    "print('Percent Explained variance:', 100*pca.explained_variance_ratio_)\n",
    "print('\\n')\n",
    "\n",
    "print(dfpca.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1e5b5612eb8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e5b572a320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = ['red', 'blue']\n",
    "\n",
    "for label, color in zip(dfpca['target'].unique(), colors):\n",
    "    mask = dfpca['target']==label\n",
    "    plt.scatter(dfpca[mask]['pc1'], dfpca[mask]['pc2'], c=color, label=label, alpha=0.5)\n",
    "plt.legend(['ALL', 'AML'])\n",
    "plt.xlabel (\"1st Principal Component\")\n",
    "plt.ylabel(\"2nd Principal Component\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = data_train['Cancer_type'].values\n",
    "X_train = data_train[['D29963_at']].values\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "\n",
    "y_test = data_test['Cancer_type'].values\n",
    "X_test = data_test[['D29963_at']].values\n",
    "y_test = y_test.reshape(len(y_test), 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b): Linear Regression vs. Logistic Regression\n",
    "\n",
    "Begin by analyzing the differences between using linear regression and logistic regression for classification. For this part, you shall work with a single gene predictor: `M23161_at`.\n",
    "\n",
    "1. Fit a simple linear regression model to the training set using the single gene predictor `D29963_at`. We could interpret the scores predicted by regression model interpreted for a patient as an estimate of the probability that the patient has the `ALL` type cancer (class 1). Is there a problem with this interpretation?\n",
    "\n",
    "2. The fitted linear regression model can be converted to a classification model (i.e. a model that predicts one of two binary labels 0 or 1) by classifying patients with predicted score greater than 0.5 into the `ALL` type (class 1), and the others into the `AML` type (class 0). Evaluate the classification accuracy (1 - misclassification rate) of the obtained classification model on both the training and test sets.\n",
    "\n",
    "3. Next, fit a simple logistic regression model to the training set. How does the training and test calssification accuracy of this model compare with the linear regression model?  Remember, you need to set the regularization parameter for sklearn's logistic regression function to be a very large value in order not to regularize (use 'C=100000').\n",
    "\n",
    "4. Plot the quantitative output from linear regression model and the probabilistic output from the logistic regression model (on the training set points) as a function of the gene predictor. Also, display the true binary response for the training set points in the same plot.\n",
    "Based on these plots, does one of the models appear better suited for binary classification than the other? Explain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAn interpretation that the regression model score is a probability that the patient has ALL type cancer would\\nnot make sense because linear regression is not suitable for classification type application. Plus, some values for\\nD29963_at could well result in a negative probability.\\n\\nThe logistic regression is clearly better than the linear regression for predicting the presence of cancer. Classification\\naccuracy was 83% in the test case with five false negative for the OLS regression. While this result is better than a coin\\nflip, the logistic regression provided 92.7% accuracy.\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "An interpretation that the regression model score is a probability that the patient has ALL type cancer would\n",
    "not make sense because linear regression is not suitable for classification type application. Plus, some values for\n",
    "D29963_at could well result in a negative probability.\n",
    "\n",
    "The logistic regression is clearly better than the linear regression for predicting the presence of cancer. Classification\n",
    "accuracy was 83% in the test case with five false negative for the OLS regression. While this result is better than a coin\n",
    "flip, the logistic regression provided 92.7% accuracy.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The equation of the regression using single gene predictor D29963_at is: [ 0.13126285] + [[ 0.75087457]]x\n",
      "\n",
      "\n",
      "Training data:\n",
      "\n",
      "False Positives:  1\n",
      "False Negatives:  8\n",
      "Correct Assessment:  23\n",
      "Classification Accuracy:  0.71875\n",
      "\n",
      "\n",
      "Testing data:\n",
      "\n",
      "False Positives:  2\n",
      "False Negatives:  5\n",
      "Correct Assessment:  34\n",
      "Classification Accuracy:  0.8292682926829268\n"
     ]
    }
   ],
   "source": [
    "def leppard(source_data, prediction_data):\n",
    "    false_negative = 0\n",
    "    false_positive = 0\n",
    "    correct_assessment = 0\n",
    "    for result in range(0, len(prediction_data)):\n",
    "        if int(prediction_data[result]) == 1 and int(source_data[result]) == 0:\n",
    "            false_positive += 1\n",
    "        if int(prediction_data[result]) == 0 and int(source_data[result]) == 1:\n",
    "            false_negative += 1\n",
    "        if (int(prediction_data[result]) == 1 and int(source_data[result]) == 1) or (int(prediction_data[result]) == 0 and int(source_data[result]) == 0):\n",
    "            correct_assessment += 1\n",
    "    print ()\n",
    "    print (\"False Positives: \", false_positive)\n",
    "    print (\"False Negatives: \", false_negative)\n",
    "    print (\"Correct Assessment: \", correct_assessment)\n",
    "\n",
    "    print (\"Classification Accuracy: \", 1 - (false_positive + false_negative) / len(source_data))\n",
    "\n",
    "# Linear regression on D29963_at\n",
    "lm = LinearRegression(fit_intercept=True)\n",
    "lm.fit(X_train, y_train)\n",
    "lm_y_pred_train = lm.predict(X_train)\n",
    "print('The equation of the regression using single gene predictor D29963_at is: {} + {}x'.format(lm.intercept_, lm.coef_))\n",
    "\n",
    "lm.fit(X_test, y_test)\n",
    "lm_y_pred_test = lm.predict(X_test)\n",
    "\n",
    "\n",
    "results = lm.intercept_ + lm.coef_ * X_train\n",
    "results = np.round_(results, 0)\n",
    "i_results = []\n",
    "for result in range(0, len(results)):\n",
    "    if results[result] < .5:\n",
    "        i_results.append(0)\n",
    "    else:\n",
    "        i_results.append(1)\n",
    "\n",
    "# Try on the train set\n",
    "cancer_train = data_train['Cancer_type'].values\n",
    "cancer_train = cancer_train.reshape(len(cancer_train), 1)\n",
    "print('\\n')\n",
    "print('Training data:')\n",
    "#print(cancer_train)\n",
    "#print(i_results)\n",
    "leppard(cancer_train, i_results)\n",
    "\n",
    "# Now on the test set\n",
    "results = lm.intercept_ + lm.coef_ * X_test\n",
    "results = np.round_(results, 0)\n",
    "j_results = []\n",
    "for result in range(0, len(results)):\n",
    "    if results[result] < .5:\n",
    "        j_results.append(0)\n",
    "    else:\n",
    "        j_results.append(1)\n",
    "        \n",
    "cancer_test = data_test['Cancer_type'].values\n",
    "cancer_test = cancer_test.reshape(len(cancer_test), 1)\n",
    "print('\\n')\n",
    "print('Testing data:')\n",
    "leppard(cancer_test, j_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated beta1: \n",
      " [[ 3.55392665]]\n",
      "Estimated beta0: \n",
      " [-1.6981375]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test Set Confusion matrix:\n",
      "[[26  2]\n",
      " [ 5  8]]\n",
      "The training classification accuracy is:  0.71875\n",
      "The testing classification accuracy is:  0.829268292683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wlt42\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression on D29963_at\n",
    "clf = LogisticRegression(C=100000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# The coefficients\n",
    "print('Estimated beta1: \\n', clf.coef_)\n",
    "print('Estimated beta0: \\n', clf.intercept_)\n",
    "\n",
    "# Scoring\n",
    "clf_y_pred_train = clf.predict(X_train)\n",
    "clf_y_pred_test = clf.predict(X_test)\n",
    "clf_y_pred_train = clf_y_pred_train.reshape(len(clf_y_pred_train), 1)\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "#print('Training data:')\n",
    "#print(clf_y_pred_train)\n",
    "#print(i_results)\n",
    "#leppard(clf_y_pred_train, i_results)\n",
    "#print('\\n')\n",
    "#print('Testing data:')\n",
    "#leppard(clf_y_pred_test, j_results)\n",
    "\n",
    "# Metrics\n",
    "print('\\n')\n",
    "print('Test Set Confusion matrix:') \n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "\n",
    "train_score = clf.score(X_train, y_train)\n",
    "test_score = clf.score(X_test, y_test)\n",
    "print('The training classification accuracy is: ', train_score)\n",
    "print('The testing classification accuracy is: ', test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAELCAYAAADOeWEXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nicMYY0xALHEYY4wJyP8HYiHlWdkbrqIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e5b55ddf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training data\n",
    "plt.scatter(data_train[['D29963_at']], data_train['Cancer_type'],  color='black')\n",
    "\n",
    "# plot logistic \n",
    "def model(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "loss = model(X_train * clf.coef_ + clf.intercept_).ravel()\n",
    "plt.scatter(X_train, loss, color='red', linewidth=3)\n",
    "X = np.sort(X_train)\n",
    "#plt.plot(X, clf.predict(X), color='red',lw=1)\n",
    "# plot linear\n",
    "plt.plot(X_train, lm.predict(X_train), color='blue',lw=1)\n",
    "\n",
    "# Labels and such\n",
    "plt.xlabel (\"D29963_at\")\n",
    "plt.ylabel(\"Cancer Type\")\n",
    "\n",
    "plt.legend(('Linear Regression Model', 'Training Data', 'Logistic Regression'),\n",
    "           loc=\"best\", fontsize='small')\n",
    "\n",
    "\n",
    "# Alternate plot logistic \n",
    "#def model(x):\n",
    "#    return 1 / (1 + np.exp(-x))\n",
    "#loss = model(X_train * clf.coef_ + clf.intercept_).ravel()\n",
    "#plt.scatter(X_train, loss, color='red', linewidth=2)\n",
    "\n",
    "\n",
    "\n",
    "# Alternate plot training data\n",
    "#plt.scatter(X_train.ravel(), y_train, color='black', zorder=20)\n",
    "\n",
    "\n",
    "# Alternate plot linear\n",
    "#ols = linear_model.LinearRegression()\n",
    "#ols.fit(X_train, y_train)\n",
    "#plt.plot(X_train, ols.coef_ * X_train + ols.intercept_, linewidth=1)\n",
    "#plt.axhline(.5, color='.5')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (c): Multiple Logistic Regression\n",
    "\n",
    "1. Next, fit a multiple logistic regression model with all the gene predictors from the data set.  How does the classification accuracy of this model compare with the models fitted in Part (b) with a single gene (on both the training and test sets)?  \n",
    "\n",
    "2. \"Use the `visualize_prob` from `HW5_functions.py` to visualize the probabilties predicted by the fitted multiple logistic regression model on both the training and test data sets. The function creates a visualization that places the data points on a vertical line based on the predicted probabilities, with the `ALL` and `AML` classes shown in different colors, and with the 0.5 threshold highlighted using a dotted horizontal line.  Is there a difference in the spread of probabilities in the training and test plots? Are there data points for which the predicted probability is close to 0.5? If so, what can you say about these points?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy of for the model with all predictors is 100%, compared to a training score of 71.8%. This makes since in that we expect more predictors to increase training accuracy. Perhaps due to the limited number of observation, the testing score of the model with one predictor was higher than its training score at 82.9%. The testing accuracy for the secon model clearly outperformed the first, with a testing accuracy of 92.7%\n",
    "\n",
    "We had many problems implementing the visualization function. We can say that those points that fall close to the .5 line can of course be classified using a different probability threshold if it was determined that errors of one type were more costly than errors of another type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = data_train['Cancer_type'].values\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "y_test = data_test['Cancer_type'].values\n",
    "y_test = y_test.reshape(len(y_test), 1)\n",
    "\n",
    "df_train = data_train.drop('Cancer_type', axis=1)\n",
    "df_test = data_test.drop('Cancer_type', axis=1)\n",
    "X_train = np.array(df_train)\n",
    "X_test = np.array(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - TRAIN\n",
      "Estimated betas: \n",
      " [[ 0.02556793 -0.0393443   0.01185487 ...,  0.00151029  0.09575496\n",
      "   0.00958508]]\n",
      "Estimated beta0: \n",
      " [-0.00831347]\n",
      "Logistic Regression - TEST\n",
      "Training score of model 1.0\n",
      "Testing score of model 0.926829268293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wlt42\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Create logistic regression object\n",
    "logitm = LogisticRegression(C = 1000000)\n",
    "logitm.fit (X_train, y_train)\n",
    "\n",
    "# The coefficients\n",
    "print(\"Logistic Regression - TRAIN\")\n",
    "print('Estimated betas: \\n', logitm.coef_)\n",
    "print('Estimated beta0: \\n', logitm.intercept_)\n",
    "\n",
    "print(\"Logistic Regression - TEST\")\n",
    "train_score_log = logitm.score(X_train, y_train)\n",
    "test_score_log = logitm.score(X_test, y_test)\n",
    "print('Training score of model', train_score_log)\n",
    "print('Testing score of model', test_score_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM8AAADxCAYAAABs8eOCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAE/FJREFUeJzt3X2QXXV9x/H3x5gYntElVsgmslbCQ6ikZHksJaT4kECR\nQSAskmjQTgChkkpbwBKrJmOsBSeikLAKE4Q2mxAQAgQRMEAZYJpEESQQGjGwi1hJUAsyQALf/nHO\nhrtPd0/OvWf33uznNZPZex7u2e/O8OGc+7u/B0UEZrb93jXYBZjVK4fHLCeHxywnh8csJ4fHLCeH\nxyynwsIj6TpJv5P0yz6OS9KVkjZIelzSoUXVYlaEIu88i4EpZY5PBfZL/80CFhZYi1nVFRaeiHgQ\neLnMKScDP4zEo8CekvYuqh6zanv3IP7u0UB7yXZHuu/F7idKmkVyd2KXXXaZeMABBwxIgfVu7dp3\nXk+cOHh11JO1a9duiohRWc4dzPBkFhGtQCtAc3NzrFmzZpArqn0jRnTdfvxxePPNwamlnkh6Luu5\ng9na9gIwpmS7Md1nFRoxArZsgeHDISL5uWVLz0BZZQYzPCuAz6StbkcCf4yIHo9stv06g9N5p3nz\nzXcCZNVT2GObpCXAccBekjqAfwWGA0TEImAlcAKwAXgNOLuoWoaa3jrK+5Gt+goLT0Sc2c/xAM4v\n6vdbZbZs2UJHRwevv/76YJdSiJEjR9LY2Mjw4cNzX6MuGgxs4HV0dLDbbrux7777Immwy6mqiGDz\n5s10dHTQ1NSU+zrunmO9ev3112loaNjhggMgiYaGhorvqg6P9WlHDE6navxtDo9ZTg6P1bRbb70V\nSTz99NMAbNy4kYMPPrjHeTNnzmT58uUDWpvDYxX71rdg1aqu+1atSvZXasmSJRxzzDEsWbKk8otV\nmcNjFTvsMJg27Z0ArVqVbB92WGXXffXVV3nooYe49tpraWtrq7zQKnN4rGKTJ8OyZUlgvvKV5Oey\nZcn+Stx2221MmTKFcePG0dDQwNrSnq41wOGxqpg8Gc47D+bOTX5WGhxIHtlaWloAaGlpqblHN39J\nalWxahUsXAhz5iQ/J0+uLEAvv/wyP/3pT3niiSeQxFtvvYUkzj+/djql+M5jFev8jLNsGXz96+88\nwnVvRNgey5cvZ8aMGTz33HNs3LiR9vZ2mpqaaG9v7//NA8ThsYqtXt31M07nZ6DVq/Nfc8mSJZxy\nyild9p166qnMnz+f9evX09jYuO3fTTfdBMA555yzbd9RRx2V/5dnpHqbq9qD4QbGU089xYEHHjjY\nZRSqt79R0tqIaM7yft95zHJyeMxycnjMcnJ4zHJyeMxycnjMcnJ4rKb1NiRBEpdddtm2czZt2sTw\n4cO54IILAPjqV7/K5ZdfXnhtDo9VrsAxCb0NSWhqauLOO+/ctn3TTTcxfvz4in/X9nJ4rHIFjUno\na0jCzjvvzIEHHkjnl+VLly5l2rRpFf2uPBweq1xBYxLKDUloaWmhra2N9vZ2hg0bxj777FPpX7Hd\nHB6rjgLGJJQbkjBlyhTuuece2traOOOMMyr+XXl4SIJVR5XHJPQ3JGHEiBFMnDiRK664gnXr1rFi\nxYpq/SWZOTxWudIxCZ2hqfDRrXNIwjXXXLNt36RJk7oMSbjooouYNGkS73vf+yr+E/LwY5tVroAx\nCeWGJHQaP348n/3sZ3t9/7x587oMWyiChyRYrzwkoX++85jl5PCY5eTwWJ/q7ZF+e1Tjb3N4rFcj\nR45k8+bNO2SAOpcYGTlyZEXXcVO19aqxsZGOjg5eeumlwS6lEJ2LW1XC4bFeDR8+vKKFn4YCP7aZ\n5VRoeCRNkbRe0gZJl/RyfA9Jt0v6haQnJXlRX6sbhYVH0jDgKmAqcBBwpqSDup12PrAuIg4hWTn7\nCkkjiqrJrJqKvPMcDmyIiGcj4k2gDTi52zkB7KZkjbtdgZeBrQXWZFY1RYZnNFA6sXBHuq/U94AD\ngd8ATwAXRsTb3S8kaZakNZLW7KitP1Z/BrvB4BPAY8A+wATge5J2735SRLRGRHNENI8aNWqgazTr\nVZHheQEYU7LdmO4rdTZwSyQ2AL8GDiiwJrOqKTI8q4H9JDWljQAtQPcRS88DxwNI+jNgf+DZAmsy\nq5rCviSNiK2SLgDuBoYB10XEk5LOTY8vAuYCiyU9AQi4OCI2FVWTWTUV2sMgIlYCK7vtW1Ty+jfA\nx4uswawog91gYFa3HB6znPoNj6S/krRL+nq6pG9L+mDxpZnVtix3noXAa5IOAS4CfgX8sNCqzOpA\nlvBsjWRE1MnA9yLiKmC3Yssyq31ZWttekXQpMB04VtK7gOHFlmVW+7Lcec4A3gA+HxG/Jekp8O+F\nVmVWBzLdeYDvRMRbksaRdJ9Z0s97zHZ4We48DwLvkTQa+AkwA1hcZFFm9SBLeBQRrwGfAq6OiNOB\ng4sty6z2ZQqPpKOAs4DO5bj85aoNeVlCcCFwKfCjtGPnh4BV/bzHbIfXb4NBRDxI8rmnc/tZ4ItF\nFmVWD/oNj6RRwD8D44FtUyxGxN8UWJdZzcvy2PYfwNNAE/A1YCPJQDezIS1LeBoi4lpgS0Q8EBGf\nA3zXsSEvy5ekW9KfL0o6kWSmm8FZx86shmQJzzxJe5D0qP4usDvwD4VWZVYHsrS23ZG+/CNQ+frg\nZjuIPsMj6bskM3r2KiLcXG1DWrk7j1fNNSujz/BExPUDWYhZvckyh8E9kvYs2X6vpLuLLcus9mX5\nnmdURPyhcyMifg+8v7iSzOpDlvC8JWls50Y6c86Ot8qr2XbK8j3PvwAPSXqAZErcvwZmFVqVWR3I\n8j3PjyUdChyZ7prt+aTNMs5VnYbljn5PNBtCPCLULCeHxyynXOGR9Hy1CzGrN3nvPKpqFWZ1KG94\n/D2PDXnlelV/qa9DwK7FlGNWP8rdeXbr49+uwHeyXFzSFEnrJW2QdEkf5xwn6TFJT6ZfxJrVhXK9\nqr9WyYUlDQOuAj4GdACrJa2IiHUl5+wJXA1MiYjnJbnPnNWNco9tV5Z7Y4bBcIcDG9J53pDURrLG\nz7qScz4N3BIRz6fX/F2Wos1qQbkeBucCvwSWkUz6sb0tbKOB9pLtDuCIbueMA4ZLup/kkfA7EdFj\n1TlJs0j7040dO7b7YbNBUS48ewOnk6zPsxVYCiwvHZ5Qpd8/ETge2Al4RNKjEfFM6UkR0Qq0AjQ3\nN7ulz2pCnw0GEbE5IhZFxGTgbGBPYJ2kGRmv/QIwpmS7Md1XqgO4OyL+lPafexA4JHP1ZoMoy0jS\nQ0kme58O3AWszXjt1cB+kpokjQBagBXdzrkNOEbSuyXtTPJY91TW4s0GU7kGg68DJ5L8x9wGXBoR\nW7NeOCK2SroAuBsYBlyXrrJwbnp8UUQ8JenHwOPA28APIuKX+f8cs4GjZKHrXg5IbwO/Bl5Ld3We\nKCAi4iPFl9dTc3NzrFnjiX2sGJLWRkRzlnPLNRg0Vakesx1SuS9Jn+ttv6RjgDOB84sqyqweZBpJ\nKukvSb7QPJ3kUe6WIosyqwflGgzGkdxhzgQ2kXzPo7Tp2mzIK3fneRr4L+BvI2IDgCSvjmCWKvc9\nz6eAF4FVkr4v6Xg8CM5sm3I9DG6NiBbgAJLVr2cD75e0UNLHB6pAs1rVbw+DtOvMf0bESSRdbH4O\nXFx4ZWY1bruGYUfE7yOiNSKOL6ogs3rhqafMcuozPJLeM5CFmNWbcneeRwAk3TBAtZjVlXLf84yQ\n9GngaEmf6n4wItzLwIa0/oZhn0UyCO6kbscCd9GxIa5cx9CHSNblWRMR1w5gTWZ1IUvH0BskfRE4\nNt1+AFgUEVuKK6tv69ev57jjjuuyb9q0aXzhC1/gtdde44QTTujxnpkzZzJz5kw2bdrEaaed1uP4\neeedxxlnnEF7ezszZvQcZX7RRRdx0kknsX79es4555wexy+77DI++tGP8thjjzF79uwex7/xjW9w\n9NFH8/DDD/PlL3+5x/EFCxYwYcIE7r33XubNm9fj+DXXXMP+++/P7bffzhVXXNHj+A033MCYMWNY\nunQpCxcu7HF8+fLl7LXXXixevJjFixf3OL5y5Up23nlnrr76apYtW9bj+P333w/A5Zdfzh13dF1p\nZqedduKuu+4CYO7cudx3331djjc0NHDzzTcDcOmll/LII490Od7Y2MiNN94IwOzZs3nssce6HB83\nbhytra0AzJo1i2ee6TK9BRMmTGDBggUATJ8+nY6Oji7HjzrqKObPnw/AqaeeyubNm7scP/7445kz\nZw4AU6dO7fG3l5MlPFcDw9OfADOAhcDfbddvMtvB9DmSdNsJ0i8i4pD+9g0UjyS1Im3PSNKsC/r+\necnFPwS8lbc4sx1Flse2fyLpWf0sSa/qD5JMRWU2pGVZ0Pc+SfsB+6e71kfEG8WWZVb7si7o+wbJ\n9FBmlnLHULOcHB6znLJMt3uLpBMlOWhmJbIE4mqSaaf+R9I3Je3f3xvMhoIsw7DvjYizgEOBjcC9\nkh6WdLak4UUXaFarMj2KSWoAZpJ0yfk5yZqkhwL3FFaZWY3rt6la0o9IvuO5ATgpIl5MDy2V5H4y\nNmRl+Z7n+xGxsnSHpPdExBtZ+wCZ7YiyPLb17COfDtE2G8rKzVX9AZJFeXdKJ3rvnC10d2DnAajN\nrKaVe2z7BEkjQSPw7ZL9rwA9R3SZDTHlhmFfD1wv6dSIuHkAazKrC+Ue26ZHxI3AvpK+1P14RHy7\nl7eZDRnlGgx2SX/uCuzWy79+SZoiab2kDZIuKXPeYZK2Suo5wYBZjSr32HZN+vNreS4saRhwFfAx\noANYLWlFRKzr5bx/A36S5/eYDZZyj21XlntjRHyxn2sfDmyIiGfT67UBJwPrup3398DNwGH9VmtW\nQ8q1tq2t8NqjgfaS7Q7giNITJI0GTgEmUyY8kmYBswDGjh1bYVlm1dFfa1vRFgAXR8TbUt+LzkVE\nK9AKyew5A1CXWb/KPbYtiIjZkm4nmV63i4j4ZD/XfgEYU7LdmO4r1Qy0pcHZCzhB0taIuDVL8WaD\nqdxjW+fqCJfnvPZqYD9JTSShaSEZF7RNRDR1vpa0GLjDwbF6Ue6xbW368wFJI0jWJg2S2XPe7O/C\nEbFV0gXA3cAw4LqIeFLSuenxRdX4A8wGS5YhCScCi4BfkfRva5J0TkTc1d97097YK7vt6zU0ETEz\nS8FmtSLLkIQrgMkRsQEgnT30TqDf8JjtyLIMSXilMzipZ0k6h5oNaeVa2zpXg1sjaSWwjOQzz+kk\njQFmQ1q5x7bS1eD+F5iUvn4J2KmwiszqRLnWNk/mblZGlta2kcDngfHAyM79EfG5Ausyq3lZGgxu\nAD5AMrL0AZKeAm4wsCEvS3g+HBFzgD+l/d1OpFsHT7OhKEt4Ohfu/YOkg4E9gPcXV5JZfcjyJWmr\npPcCc4AVJCNL5xRalVkdyLIy3A/Slw8AHyq2HLP6kWWJkQZJ35X0M0lrJS1I5642G9KyfOZpA34H\nnAqcBmwClhZZlFk9yPKZZ++ImFuyPU/SGUUVZFYvstx5fiKpRdK70n/TSMbomA1p5TqGvkLSEVTA\nbODG9NC7gFeBfyy8OrMaVq5vW6aJDc2GqiyfeZD0SeDYdPP+iLijuJLM6kOWpupvAheSTFa4DrhQ\n0vyiCzOrdVnuPCcAEyLibQBJ15OsS3ppkYWZ1bpMC/oCe5a83qOIQszqTZY7z3zg55JWkbS8HQv0\nueKB2VBRNjxKpvJ8CDiSd+aSvjgiflt0YWa1rmx4IiIkrYyIvyDpUW1mqSyfeX4myct/mHWT5TPP\nEcB0SRuBP5F87omI+EiRhZnVuizh+UThVZjVoXJ920YC5wIfBp4Aro2IrQNVmFmtK/eZ53qS9XOe\nAKaSzFltZqlyj20Hpa1sSLoW+O+BKcmsPpS783TOmoMf18x6KnfnOUTS/6WvBeyUbne2tu1eeHVm\nNazceJ5hA1mIWb3J2jHUzLopNDySpkhaL2mDpB6dSSWdJelxSU9IeljSIUXWY1ZNhYVH0jDgKpJm\n7oOAMyUd1O20XwOT0la9uUBrUfWYVVuRd57DgQ0R8Wy6enYbcHLpCRHxcET8Pt18lGQFBrO6UGR4\nRgPtJdsd6b6+fJ4+FgmWNEvSGklrXnrppSqWaJZfTTQYSJpMEp6LezseEa0R0RwRzaNGjRrY4sz6\nkGn2nJxeAMaUbDem+7qQ9BHgB8DUiNhcYD1mVVXknWc1sJ+kJkkjgBa6DaiTNBa4BZgREc8UWItZ\n1RV254mIrZIuIJmadxhwXUQ8Kenc9Pgi4CtAA3B1MuKbrRHRXFRNZtWkiBjsGrZLc3NzrFmzZrDL\nsB2UpLVZ/wdeEw0GZvXI4THLyeExy8nhMcvJ4THLyeExy8nhMcvJ4THLyeExy8nhMcvJ4THLyeEx\ny8nhMcvJ4THLyeExy8nhMcvJ4THLyeExy8nhMcvJ4THLyeExy8nhMcvJ4THLyeExy8nhMcvJ4THL\nyeExy8nhMcvJ4THLyeExy8nhMcvJ4THLyeExy8nhMcvJ4THLqdDwSJoiab2kDZIu6eW4JF2ZHn9c\n0qFF1jNkSMm//vZZRQoLj6RhwFXAVOAg4ExJB3U7bSqwX/pvFrCwqHqGpM6wODSFKPLOcziwISKe\njYg3gTbg5G7nnAz8MBKPAntK2rvAmoaG0hXOS4NTZyuf17p3F3jt0UB7yXYHcESGc0YDL5aeJGkW\nyZ0J4FVJ66tb6o5pIkzsfL0W1voOlMkHs55YZHiqJiJagdbBrsOsVJGPbS8AY0q2G9N923uOWU0q\nMjyrgf0kNUkaAbQAK7qdswL4TNrqdiTwx4h4sfuFzGpRYY9tEbFV0gXA3cAw4LqIeFLSuenxRcBK\n4ARgA/AacHZR9ZhVm8ItMGa5uIeBWU4Oj1lODo9ZTg6PWU4Oj1lODo9ZTg6PWU7/D50XBGEnx/Z+\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e5b644c6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(121)\n",
    "# starter code\n",
    "y_train_viz = data_train[\"Cancer_type\"]\n",
    "y_test_viz = data_test[\"Cancer_type\"]\n",
    "\n",
    "# Note: We tried various ways to get this function to work, but in the end it would not\n",
    "# and we are not sure why.\n",
    "visualize_prob(logitm, X_train, y_train_viz, ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (d): Analyzing Significance of Coefficients\n",
    "\n",
    "How many of the coefficients estimated by the multiple logistic regression in the previous problem are significantly different from zero at a *significance level of 95%*? \n",
    "\n",
    "Hint: To answer this question, use *bootstrapping* with 100 boostrap samples/iterations.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found 956 predictors whose coefficients are significantly different than 0 at the level of 95%. \n",
    "A dataframe below lists these predictors along with the results of the bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FROM PREVIOUS HOMEWORK\n",
    "# randomly sample our data\n",
    "\n",
    "def sample(x, y, k):\n",
    "    n = x.shape[0] # No. of training points\n",
    "    \n",
    "    # Choose random indices of size 'k'\n",
    "    subset_ind = np.random.choice(np.arange(n), k)\n",
    "    \n",
    "    # Get predictors and reponses with the indices\n",
    "    x_subset = x[subset_ind, :]\n",
    "    y_subset = y[subset_ind]\n",
    "   \n",
    "    return (x_subset, y_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wlt42\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap 100 random samples from population\n",
    "\n",
    "coeff_dict = {}\n",
    "tdict = {}\n",
    "\n",
    "for i in range(0, 100):\n",
    "    \n",
    "    sample_X, sample_y = sample(X_train, y_train, 22)\n",
    "    \n",
    "    # Create logistic regression object\n",
    "    logit = LogisticRegression(C = 1000000)\n",
    "    logit.fit(sample_X, sample_y)\n",
    "\n",
    "    # The coefficients\n",
    "    row = logit.coef_[0] \n",
    "    tdict = {i : row}\n",
    "    coeff_dict.update(tdict)\n",
    "   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for our hundred resamples\n",
    "\n",
    "df = pd.DataFrame(coeff_dict)\n",
    "df = df.T\n",
    "df.columns = df_.columns\n",
    "df = df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute statistics from bootstrapped samples\n",
    "\n",
    "df['Mean'] = df.mean(axis=1)\n",
    "df['STD'] = df.std(axis=1)\n",
    "df['CI Lower'] = df['Mean'] - 1.96* df['STD']\n",
    "df['CI Upper'] = df['Mean'] + 1.96* df['STD']\n",
    "df['Standard Error'] = (df['CI Upper'] - df['CI Lower'])/(2*1.96)\n",
    "df['Variance'] = df['STD'] * df['STD']\n",
    "df['t stat'] = df['Mean']/df['Standard Error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_significant = df[~((df['CI Upper']>0) & (df['CI Lower']<0))]\n",
    "df_significant = df_significant[['Mean','STD','Standard Error','Variance','t stat','CI Lower','CI Upper']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                AFFX-BioB-3_st  AFFX-HUMISGF3A/M97935_MA_at  AFFX-M27830_5_at  \\\n",
      "Mean                 -0.020462                     0.024847         -0.027560   \n",
      "STD                   0.009457                     0.010593          0.011174   \n",
      "Standard Error        0.009457                     0.010593          0.011174   \n",
      "Variance              0.000089                     0.000112          0.000125   \n",
      "t stat               -2.163767                     2.345592         -2.466339   \n",
      "\n",
      "                AFFX-M27830_M_at  AB000114_at  AB000449_at  AB000905_at  \\\n",
      "Mean                   -0.026957    -0.036298    -0.025948    -0.038268   \n",
      "STD                     0.009387     0.017119     0.009702     0.015890   \n",
      "Standard Error          0.009387     0.017119     0.009702     0.015890   \n",
      "Variance                0.000088     0.000293     0.000094     0.000252   \n",
      "t stat                 -2.871751    -2.120340    -2.674580    -2.408328   \n",
      "\n",
      "                AB002559_at  AC000061_cds2_at  AC000064_cds1_at    ...      \\\n",
      "Mean               0.041198         -0.044799          0.038169    ...       \n",
      "STD                0.015626          0.020885          0.017681    ...       \n",
      "Standard Error     0.015626          0.020885          0.017681    ...       \n",
      "Variance           0.000244          0.000436          0.000313    ...       \n",
      "t stat             2.636500         -2.145025          2.158816    ...       \n",
      "\n",
      "                J00148_cds2_f_at  K03189_f_at  M60750_f_at  M77481_rna1_f_at  \\\n",
      "Mean                   -0.026788     0.026989    -0.015814         -0.054124   \n",
      "STD                     0.012217     0.011250     0.007793          0.018501   \n",
      "Standard Error          0.012217     0.011250     0.007793          0.018501   \n",
      "Variance                0.000149     0.000127     0.000061          0.000342   \n",
      "t stat                 -2.192655     2.398918    -2.029193         -2.925456   \n",
      "\n",
      "                X13930_f_at  X71345_f_at  Z80780_f_at  U88902_cds1_f_at  \\\n",
      "Mean              -0.027646    -0.029313    -0.034829         -0.017790   \n",
      "STD                0.012757     0.014149     0.016612          0.008123   \n",
      "Standard Error     0.012757     0.014149     0.016612          0.008123   \n",
      "Variance           0.000163     0.000200     0.000276          0.000066   \n",
      "t stat            -2.167138    -2.071674    -2.096570         -2.190080   \n",
      "\n",
      "                L78833_cds4_at  U29175_at  \n",
      "Mean                 -0.027995  -0.045730  \n",
      "STD                   0.009828   0.013306  \n",
      "Standard Error        0.009828   0.013306  \n",
      "Variance              0.000097   0.000177  \n",
      "t stat               -2.848574  -3.436761  \n",
      "\n",
      "[5 rows x 980 columns]\n"
     ]
    }
   ],
   "source": [
    "# Dataframe of all predictors significantly different from 0 at the level of 95% \n",
    "\n",
    "temp_df = df_significant.T\n",
    "significant_pred = list(temp_df.columns)\n",
    "print(temp_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_significant = data_train[significant_pred].values\n",
    "y_train_significant = data_train['Cancer_type'].values\n",
    "X_test_significant = data_test[significant_pred].values\n",
    "y_test_significant = data_test['Cancer_type'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated betas: \n",
      " [[ -7.61590382e-02   7.48193488e-02  -7.32091966e-02  -7.27898245e-02\n",
      "   -1.07971652e-01  -7.35103765e-02  -1.13040374e-01   1.21231152e-01\n",
      "   -1.28047529e-01   1.12081841e-01  -8.41125034e-02  -1.31390396e-01\n",
      "   -6.52983650e-02  -5.50626296e-02  -1.31008026e-01  -1.20419402e-01\n",
      "    3.45985110e-01   7.20597746e-02  -1.65158995e-01  -1.66011118e-01\n",
      "    1.66516128e-01  -1.36924555e-02   1.39216155e-01   1.00409899e-01\n",
      "    5.07245227e-02  -1.56813837e-01   5.69712170e-02  -7.71137726e-02\n",
      "    2.67081183e-01   4.21511711e-02  -9.51858769e-02  -7.88464426e-02\n",
      "    2.38926411e-01   1.04011324e-01  -1.54133078e-01  -5.61290155e-02\n",
      "   -1.93947934e-01  -4.33288671e-02  -4.19758443e-02  -9.62206850e-02\n",
      "   -3.69377509e-02  -2.05217634e-01  -2.94901142e-01   9.74104101e-02\n",
      "    1.41671566e-01   3.00172975e-01  -9.29071179e-02   7.03819068e-02\n",
      "   -1.08523360e-01  -4.41833008e-02  -2.40350975e-01  -4.67852640e-02\n",
      "   -3.39258286e-02   1.04057712e-01  -1.02928715e-01  -1.48868195e-01\n",
      "    1.53980514e-01  -3.60736944e-02   6.93578560e-02   1.25335588e-01\n",
      "   -6.58807370e-02  -8.00891273e-02  -1.74524211e-01   5.41725102e-02\n",
      "   -5.42137832e-02  -1.41320334e-01  -8.21224413e-02  -2.51740163e-01\n",
      "   -7.23535312e-02  -8.94087277e-02  -3.93691358e-02  -2.14189545e-02\n",
      "   -1.35342539e-01  -4.30497350e-02  -8.91201038e-02   1.54112148e-01\n",
      "   -1.22681214e-01  -2.03442870e-01  -8.09335269e-02  -1.28814550e-01\n",
      "    9.30791474e-02   1.31955609e-01  -9.75110975e-02  -2.06579930e-01\n",
      "   -2.60134608e-02  -1.73175104e-01   1.18825913e-01   1.36177897e-01\n",
      "    1.71239073e-01  -3.02015630e-02   3.88427057e-02  -6.94436990e-02\n",
      "   -2.81976161e-01  -1.48792021e-02   2.59846876e-01  -7.30128629e-02\n",
      "   -5.66304376e-02   1.44421163e-01  -5.02312666e-02  -1.63688089e-02\n",
      "    6.22609326e-02   1.33078846e-01  -1.21750724e-01  -4.99152656e-02\n",
      "   -1.20158543e-01   1.67548425e-01  -7.88268757e-02  -5.44795891e-02\n",
      "   -1.22494943e-01   1.11285124e-01  -1.44826298e-01   3.17401246e-02\n",
      "    1.18030390e-01  -4.02755281e-02  -8.25243114e-02  -6.56260347e-02\n",
      "    1.43820760e-01   1.32105372e-01   2.35983508e-01   1.26333769e-01\n",
      "   -1.25395914e-01  -2.43713976e-02  -1.14980160e-01  -6.05669386e-02\n",
      "   -1.05017950e-01   1.15371463e-01   1.09167993e-01  -1.00657197e-01\n",
      "   -7.94717318e-02  -7.47878359e-02  -2.59429536e-01   2.12848286e-01\n",
      "   -2.60951809e-01  -1.09529817e-01  -1.71951280e-01  -3.40305792e-02\n",
      "   -6.55945952e-02  -1.27428011e-01  -1.75274957e-01  -2.97245603e-01\n",
      "    1.28820081e-01  -8.67068710e-02   9.70577933e-02   9.02922231e-02\n",
      "   -2.35241406e-01  -2.36120224e-02   1.75443609e-01  -1.52864112e-01\n",
      "   -2.17876369e-03  -4.89234008e-02   1.63984308e-01   1.69109878e-01\n",
      "    2.51908918e-01   1.42169245e-01  -2.17437935e-01  -7.84730369e-02\n",
      "    9.25723253e-02  -4.57963293e-02  -1.03032101e-01   9.38385984e-02\n",
      "   -3.89494558e-02   1.44243448e-01  -5.00130469e-03  -1.71817722e-01\n",
      "   -9.15153289e-02   2.89356758e-01  -1.10873955e-01  -2.20584657e-01\n",
      "   -1.30783324e-01   1.78658844e-01  -1.31444011e-02  -1.05290581e-01\n",
      "   -6.07999133e-02  -1.64689413e-01  -9.50115873e-02   1.67511505e-01\n",
      "   -2.09025453e-01   1.20189422e-01  -1.32704833e-01   2.32851056e-01\n",
      "    7.54992049e-02  -2.14905805e-01  -4.38358942e-02  -1.69226986e-01\n",
      "   -1.28922966e-01  -1.04103193e-01  -1.34697302e-01  -1.12529400e-01\n",
      "    5.45735472e-02  -3.07152317e-02   7.39064429e-02  -2.14397888e-01\n",
      "    6.68574138e-02  -1.33453112e-01  -4.21731739e-02  -6.31017128e-02\n",
      "    9.10474083e-02  -4.18171920e-02  -1.93313724e-02  -1.09516483e-01\n",
      "    4.04080641e-02   4.13080166e-02  -1.17843544e-01  -1.09786179e-01\n",
      "   -5.86879337e-02  -1.72334162e-01  -7.85352030e-02  -5.00285962e-02\n",
      "    3.53940587e-02   2.74740589e-02  -1.95694973e-01  -2.45145366e-01\n",
      "   -1.10054432e-01   1.12936286e-01   6.25006340e-02  -1.22158036e-01\n",
      "    1.95169481e-01   1.25597240e-01   2.08093052e-01   5.05343935e-02\n",
      "    1.79210769e-01  -1.25046409e-01   1.73440819e-01   1.78859652e-01\n",
      "    2.86201153e-01   1.16130821e-01  -1.25280390e-01   1.70518469e-01\n",
      "    8.97056516e-02   3.81619143e-01   7.02384420e-02   2.01027291e-01\n",
      "    1.13487431e-01   1.96703535e-01  -3.37326571e-02   2.23448863e-01\n",
      "    1.67234913e-03   2.48644449e-01   9.12930265e-02  -3.34885073e-02\n",
      "   -3.09130213e-01   4.97109781e-02   2.40247507e-01  -8.14222687e-02\n",
      "   -1.07691068e-01  -5.15460634e-02  -7.77182311e-02   1.60225842e-01\n",
      "   -6.56781394e-02  -2.01073802e-01  -1.02681304e-01  -4.63820228e-02\n",
      "    1.73122621e-01   9.48031600e-02  -2.04011509e-01   3.62081148e-02\n",
      "    2.23886340e-01  -1.39574070e-01   6.40971759e-02  -6.70737871e-02\n",
      "    1.39279366e-01   2.24076870e-01  -2.06832493e-01   1.12784848e-01\n",
      "   -3.21357034e-02   5.23321410e-02   2.62012274e-01  -2.54887056e-01\n",
      "   -5.47914348e-02  -1.78593891e-01   9.21495447e-02  -1.77649404e-01\n",
      "   -1.38475649e-01  -3.54511942e-02  -7.38326554e-02  -8.44733209e-02\n",
      "    1.01983215e-01   1.02903906e-01   7.67695993e-02  -1.26377135e-01\n",
      "   -6.17526594e-04   7.46282941e-02  -8.51596637e-02   1.57140412e-01\n",
      "    5.01286700e-02   7.34535868e-02   3.44085992e-01   3.31682901e-02\n",
      "   -1.15946208e-01   2.30997042e-01   2.22632193e-01   7.07201678e-02\n",
      "   -2.82181539e-01   1.56139789e-01  -1.61649568e-01   2.86715565e-01\n",
      "    2.07022427e-01  -2.63910676e-01   1.70951963e-01   8.13439873e-02\n",
      "   -8.91776355e-02   3.11717240e-01   2.22964227e-01   1.36100187e-01\n",
      "   -2.20737819e-02  -5.48871874e-02   9.87433452e-02   1.54762362e-01\n",
      "    9.47455843e-02  -2.58849226e-02  -8.49112366e-02   1.27805397e-01\n",
      "   -4.72907620e-02   1.11954348e-01   1.81303637e-01   1.92915809e-01\n",
      "   -3.78390161e-01   1.41817038e-01  -1.36483760e-01  -8.60137761e-02\n",
      "    4.02893164e-02  -5.88225518e-05  -3.19530383e-02  -2.57039282e-01\n",
      "   -3.25162720e-02   2.28841752e-01  -6.06790546e-02  -8.82019619e-03\n",
      "    1.55921780e-01   1.34271818e-01   2.08309090e-01  -9.75033815e-02\n",
      "   -3.08380427e-01  -1.62945455e-01  -2.07078637e-01   1.52557995e-01\n",
      "   -1.98209054e-01  -1.08977504e-01  -1.25340508e-01  -1.80933012e-01\n",
      "   -2.59504533e-01  -1.51705045e-01  -1.84985847e-01  -3.19908814e-02\n",
      "    1.13343385e-01  -5.79962546e-02  -1.03079637e-01  -1.08701395e-01\n",
      "   -8.25871378e-02  -3.02899092e-01  -4.42169694e-03   2.20365283e-01\n",
      "    1.38565028e-01   2.34161328e-01  -9.99289011e-02   1.21985782e-01\n",
      "   -1.51804976e-01   1.25534327e-01  -2.96561207e-02  -9.01296836e-02\n",
      "   -1.78417825e-01  -2.21527075e-01   9.39899110e-02   1.93347403e-01\n",
      "    1.70434056e-01  -1.68211808e-01   2.81511724e-02  -5.42987280e-02\n",
      "   -1.43502150e-01  -1.26544919e-01  -1.29991774e-02  -1.16366237e-01\n",
      "   -1.62993561e-01  -1.45932714e-01  -3.37647938e-02  -1.33908059e-01\n",
      "   -2.58678272e-01   7.86513958e-02   1.29186637e-01  -6.25100694e-02\n",
      "   -5.49290026e-02   1.27318761e-01  -2.42650451e-01  -6.79769048e-02\n",
      "   -2.04154146e-02  -1.01279772e-01   1.57890338e-01  -9.34593724e-02\n",
      "   -8.36376788e-02   2.03990018e-01  -1.23240540e-01   7.73949020e-02\n",
      "    1.67391993e-01  -1.39856000e-01   3.41258364e-01   2.07998908e-01\n",
      "    2.13979634e-01   8.08002148e-02  -1.90649323e-01   1.44983728e-01\n",
      "   -6.04138118e-02  -8.96558421e-02  -1.31519243e-01  -2.18531476e-02\n",
      "    2.46328218e-01   1.86786769e-01   9.45855881e-02  -1.64971181e-02\n",
      "    1.20074553e-01  -2.00850318e-01   1.24650085e-01  -1.86898170e-01\n",
      "   -1.51580150e-01  -1.61704887e-01   2.00704269e-01   1.00721384e-01\n",
      "   -1.44067049e-01  -2.16254215e-01  -2.46914535e-01   7.68664154e-02\n",
      "    4.42027997e-02   2.44814356e-01   5.23304698e-02  -1.75227088e-01\n",
      "    8.83891550e-02   1.89583605e-01  -2.78729673e-01   2.11843435e-01\n",
      "   -1.42156372e-01  -2.32205083e-01  -4.64024938e-02   1.60303186e-01\n",
      "   -1.37587675e-01   2.27431723e-02   6.55359092e-02  -1.28280098e-01\n",
      "    3.51228906e-02  -2.01475177e-01  -1.33610317e-01   6.67922664e-02\n",
      "   -3.27100260e-01  -1.51211846e-01   1.56474835e-01   8.75219841e-02\n",
      "    3.78310630e-02   1.88245749e-01   3.94161047e-02  -1.81175725e-01\n",
      "   -1.71979536e-01   1.72576021e-01  -1.06691596e-01   4.17457547e-02\n",
      "    2.57898742e-01   9.49932631e-02   1.03436437e-01  -1.37635304e-01\n",
      "   -1.33711834e-01  -2.89426368e-02   1.74164769e-01   7.27475005e-02\n",
      "    2.48271113e-01   7.56991248e-02   5.12151764e-02  -2.45425861e-02\n",
      "   -7.79224390e-02   1.52520029e-01  -2.39817467e-01   1.02881323e-01\n",
      "    8.19656040e-02  -1.05637657e-01  -1.38959441e-01  -1.67085569e-02\n",
      "   -9.99158304e-02  -1.22510964e-01  -2.89426604e-01   1.62015952e-01\n",
      "   -1.98130364e-01  -2.25411973e-01   1.31183915e-01   1.79504396e-02\n",
      "   -2.98393073e-02   1.53293970e-01  -1.27483155e-01   1.78777113e-01\n",
      "   -1.55783237e-01   1.19008277e-01  -3.43636816e-02  -6.64659895e-02\n",
      "   -3.00927401e-01  -1.01227834e-01   9.53184351e-02   1.54820462e-01\n",
      "   -1.57747398e-01   1.17572755e-01  -1.51671292e-01  -1.29372585e-01\n",
      "   -3.91578879e-03  -1.00119009e-01   9.97049036e-02  -2.59375847e-01\n",
      "    6.24771296e-02  -2.26860495e-01  -4.88434986e-02  -4.30644439e-02\n",
      "   -6.01775263e-02   1.83013864e-01   2.84537535e-01   1.11759313e-01\n",
      "   -8.64157871e-02  -1.36279080e-01  -1.04930795e-01   1.17319196e-01\n",
      "   -2.16955931e-02  -1.64941464e-01   2.65647318e-01  -1.08507934e-01\n",
      "   -1.60008702e-01   3.14588345e-01  -1.11440230e-01  -1.14914252e-01\n",
      "   -1.39021015e-01   2.18427688e-01  -2.31681549e-01  -6.45207312e-02\n",
      "   -6.65400798e-02  -6.21376167e-02   5.09434555e-02  -8.84649448e-02\n",
      "    1.36961852e-01  -3.39034187e-02  -1.64734312e-01  -9.48531225e-02\n",
      "    1.99916555e-01   1.10021927e-01   1.13415965e-01   2.37486790e-01\n",
      "   -6.33925229e-02  -1.43913494e-01   1.76914828e-01   1.88493739e-01\n",
      "   -1.47401324e-01   1.60583290e-01  -1.17921966e-01  -2.13141620e-02\n",
      "    2.34145362e-01  -7.21569433e-02  -2.71842487e-01  -1.03109294e-01\n",
      "    3.97663449e-02   1.30875660e-01  -8.62856129e-02  -2.85975841e-02\n",
      "   -1.09653886e-01  -5.89826531e-02  -1.12632198e-01  -9.39161974e-02\n",
      "   -1.33903863e-02  -9.99836362e-02  -7.09640931e-02  -1.25992673e-01\n",
      "    6.01323373e-02   2.02173665e-01   7.53338107e-02   2.31102321e-01\n",
      "   -2.08270288e-01  -8.63581199e-02  -1.72135979e-01   2.24667874e-01\n",
      "   -3.10494120e-01  -3.03742802e-01  -1.79189263e-01   1.01969267e-01\n",
      "   -1.06767873e-01   1.07751246e-01  -6.03490593e-02   1.77842369e-01\n",
      "    2.12864928e-01   1.22230354e-01  -1.85855371e-01  -1.20860483e-02\n",
      "    1.66279057e-02  -2.24307233e-02  -1.95202087e-02  -6.85973772e-02\n",
      "    1.57528185e-01   9.93903213e-02  -3.78180675e-02   1.55208253e-01\n",
      "    1.32037932e-01   5.75069081e-02  -2.33747986e-01   2.95516143e-01\n",
      "    1.12627579e-01   4.94735871e-02   3.13444278e-02   1.01337159e-01\n",
      "    4.50121047e-02  -6.79062327e-02  -1.66728183e-01   7.58315013e-02\n",
      "   -1.67764560e-01  -2.70850062e-01   7.26054821e-02  -7.36619862e-02\n",
      "   -2.11857322e-01  -1.07753030e-01  -8.54486781e-02  -1.24008985e-01\n",
      "   -9.81964716e-02  -1.27549273e-01  -1.59256572e-01   1.90997609e-01\n",
      "   -7.92383059e-02   2.66616754e-02   2.23897143e-03   1.84568391e-01\n",
      "    9.95924293e-02   1.86868486e-01   2.34836335e-01   1.41913409e-01\n",
      "    6.38815969e-02   1.62194845e-01  -1.41963713e-02  -1.87932292e-01\n",
      "    3.14041110e-01   1.83362753e-01  -2.56555747e-02  -1.59899984e-01\n",
      "   -2.14254772e-01  -1.69285008e-01   1.35366245e-01  -9.04414662e-02\n",
      "   -6.12948338e-02  -1.79286285e-01  -8.83272328e-02  -2.58801329e-01\n",
      "    1.59799118e-01   1.44551558e-01   4.37666478e-02  -1.51139091e-01\n",
      "    1.44507760e-01  -4.85207856e-02   2.15597213e-01  -6.41168121e-02\n",
      "   -7.50981323e-02   2.63010694e-01  -9.93293014e-03  -2.43110719e-02\n",
      "   -8.45281245e-02  -2.14974193e-02   3.52347199e-01  -1.25035318e-01\n",
      "   -9.07319970e-02  -1.41125196e-01  -3.04773653e-01   7.71072016e-02\n",
      "   -1.70705796e-01  -1.10862834e-01  -1.20959576e-01  -2.34700718e-02\n",
      "    2.22794308e-01  -4.29466583e-02  -5.82220783e-02  -9.50467190e-02\n",
      "   -1.37156892e-01  -2.99542828e-01  -6.00018856e-02   2.87744487e-01\n",
      "    1.35224635e-01  -3.50818318e-02  -8.33427770e-02   1.59947130e-01\n",
      "    1.05450111e-01  -1.40843802e-01   8.06691574e-02  -1.94671824e-01\n",
      "    6.36110448e-02   2.70493292e-01  -1.06067292e-01  -1.09806262e-01\n",
      "   -4.75417774e-02  -4.21755506e-02  -1.49048038e-01  -4.58500421e-02\n",
      "    1.48105806e-01   1.20082474e-01  -2.18627812e-01   4.82177848e-02\n",
      "   -3.56565318e-02  -9.91187763e-02   1.08326916e-01  -2.50599466e-01\n",
      "   -2.01486214e-01   2.59386987e-01   2.52081545e-01   1.31655224e-01\n",
      "   -9.26041081e-02   5.56532683e-01  -2.01392085e-01  -1.28783861e-01\n",
      "   -4.42301751e-03  -1.94621165e-01   1.93874609e-01  -1.60657572e-01\n",
      "    2.49687729e-01  -3.64118250e-02   9.14884838e-02   1.39004860e-01\n",
      "    1.13736549e-01   8.63242881e-02  -4.87750098e-02   9.14289801e-02\n",
      "    1.29259407e-01  -2.63324665e-02  -1.03558674e-01   3.00972871e-01\n",
      "    2.06004024e-01  -4.27458255e-02  -1.79620283e-01  -6.17918917e-02\n",
      "   -2.27527943e-01  -1.13763930e-01   9.20173800e-02  -4.61064061e-02\n",
      "   -2.33318602e-01  -7.80779650e-02  -9.13980108e-02  -1.69290007e-01\n",
      "   -7.14527817e-02  -3.47568298e-02  -9.81575814e-02   5.60620509e-02\n",
      "    7.72266820e-02   9.21102957e-02   2.36574665e-02  -1.20647459e-01\n",
      "   -1.15149515e-01   3.45432709e-03  -8.54596077e-02   3.13531306e-01\n",
      "   -1.30695955e-01  -1.02614628e-01  -9.05734475e-02  -1.79316581e-02\n",
      "    1.21281699e-01   1.59865039e-01  -1.25181227e-02  -1.55866281e-01\n",
      "    7.37573301e-02  -8.49463453e-02  -1.10733926e-01  -1.49931427e-01\n",
      "    1.34328675e-01  -2.37611430e-01   1.00595297e-01  -1.30658527e-01\n",
      "    1.76219793e-01  -6.88608854e-03  -4.85209972e-02  -6.06498367e-02\n",
      "   -4.96801819e-02   1.51829937e-01  -2.13221390e-02  -1.03503536e-01\n",
      "   -1.67484732e-01  -5.93294104e-02  -5.02892197e-02  -1.61690042e-01\n",
      "   -1.82071428e-01   9.88971114e-02  -1.25703945e-01   1.00848818e-01\n",
      "    9.69351478e-02  -1.72226291e-01  -5.20338921e-02  -1.92495727e-01\n",
      "   -2.15999712e-02  -1.27918942e-01  -2.06021351e-02   1.15175091e-02\n",
      "    1.34371667e-01  -1.28145316e-01  -5.52653749e-02  -7.93465509e-02\n",
      "    1.51804369e-01   1.89081152e-01  -1.94420137e-01  -1.57968554e-01\n",
      "    1.26717170e-01   1.28544434e-01  -2.95408261e-02  -1.66853314e-01\n",
      "   -2.79099676e-02  -2.26801990e-02  -3.62943077e-01  -4.12962141e-02\n",
      "    4.83479093e-02  -1.28590331e-01  -2.15816173e-01   2.22397535e-01\n",
      "   -1.09752974e-01  -7.36401167e-02   4.06768973e-02  -2.54166770e-01\n",
      "    1.20645227e-01  -5.57498175e-02  -4.56046572e-02   1.72196979e-01\n",
      "   -1.35894021e-01  -1.97223900e-01  -6.91917964e-02  -4.47893916e-02\n",
      "    1.27885239e-01  -8.00234597e-02  -1.58485390e-01   1.22544421e-01\n",
      "    2.25677857e-01  -1.40143661e-01   5.79820197e-02   1.23492734e-01\n",
      "   -1.56769472e-01  -5.72626294e-02   1.91187663e-01   1.15296958e-01\n",
      "    1.49935049e-01  -1.08170115e-01  -7.20825632e-02  -8.78646694e-02\n",
      "    8.59844894e-02   1.18923693e-01   4.61019663e-02  -3.72091900e-02\n",
      "   -9.07379517e-02   3.99085576e-02   1.57853791e-01  -1.17729021e-01\n",
      "    1.74488482e-01  -7.75284146e-02  -8.98457094e-02   2.84964526e-01\n",
      "   -2.01631544e-01  -3.40280932e-02  -6.88904237e-02   1.94409121e-01\n",
      "    1.19615153e-01  -3.43942509e-02   6.10833879e-02   2.79307191e-01\n",
      "   -1.10522560e-01   5.13429476e-02  -1.46264400e-01   1.33294917e-01\n",
      "   -4.68552114e-02  -2.40022274e-01   2.41992187e-01   1.13720886e-01\n",
      "   -1.08165703e-01  -2.58053075e-01   1.34772587e-01   8.56789090e-03\n",
      "   -1.79983175e-01  -2.44451597e-01  -2.07687297e-01   4.35537811e-02\n",
      "    1.14289001e-01  -2.08384176e-01   2.18303663e-01  -2.18942760e-01\n",
      "    6.54047191e-02  -1.88204185e-01  -1.72689868e-01   1.12652967e-01\n",
      "    1.01101611e-01  -7.77082635e-02  -8.28170807e-02  -1.67459575e-01\n",
      "    6.39887781e-02   8.22208079e-02   7.77098940e-02  -1.14576495e-01\n",
      "    7.44392449e-02   2.12803498e-01   1.05218578e-01   1.91816722e-01\n",
      "   -8.75804164e-02   1.25818781e-01  -1.46737648e-01  -1.06571571e-01\n",
      "    2.25809658e-01   5.04520853e-02   2.14621244e-02   1.69853063e-01\n",
      "   -2.04075498e-01  -8.05639502e-02   2.54320748e-01   7.98805345e-02\n",
      "    6.50829416e-02   1.37823262e-02  -2.61484873e-02   1.34615091e-01\n",
      "   -6.76783138e-02  -7.11173998e-02  -2.27085656e-01  -1.83369553e-01\n",
      "   -2.30576381e-01  -6.19416744e-02  -3.22348251e-02   4.63039102e-02\n",
      "   -8.11666384e-02   1.34966781e-01   2.09679577e-01   3.35689452e-02\n",
      "    1.00905804e-01  -1.68783290e-01  -6.52166023e-02   7.44951443e-02\n",
      "   -1.57374667e-01  -3.11380378e-02   6.45459813e-02   1.29856159e-01\n",
      "   -7.42317987e-02  -4.26460655e-01  -5.47491184e-02  -1.91025947e-01\n",
      "   -8.29231349e-02   8.74084344e-02  -8.50417833e-02  -9.07041352e-02\n",
      "   -5.80065339e-02  -2.29181900e-02  -1.22779158e-01  -1.03633743e-01\n",
      "   -1.17340456e-01   2.33881984e-01  -1.54573846e-01   2.18750897e-01\n",
      "    4.89502451e-02  -1.84906831e-01   1.85026554e-02   2.46411847e-02\n",
      "   -5.49304375e-02  -1.49611140e-01  -1.74725861e-01  -9.04427967e-02\n",
      "    2.13824458e-01  -1.02004072e-01   1.30144147e-01  -3.56776673e-01\n",
      "    1.54307623e-01  -1.10432222e-01  -8.97167345e-02   7.59415737e-02\n",
      "   -7.06453302e-02   8.01171961e-02  -1.23357301e-01   2.10501365e-01\n",
      "   -9.44444444e-02   7.29730482e-02  -5.40383502e-02   1.15180492e-01\n",
      "    9.78401455e-02   7.75001761e-02  -2.00174822e-01  -1.90231778e-01\n",
      "   -1.46136571e-01  -2.03774571e-01  -4.36697311e-02  -8.44080524e-03\n",
      "    1.27761813e-01   9.74908603e-02  -1.35148148e-01   6.12095844e-02\n",
      "   -3.26701443e-02  -7.88069106e-02  -8.47408262e-02  -1.19721192e-01\n",
      "   -1.63370182e-01  -2.62168509e-02  -1.18928200e-01  -1.59319623e-01]]\n",
      "Estimated beta0: \n",
      " [-0.01850948]\n",
      "Training score of model 1.0\n",
      "Testing score of model 0.90243902439\n"
     ]
    }
   ],
   "source": [
    "# Create logistic regression object\n",
    "logit_significant = LogisticRegression(C = 1000000)\n",
    "logit_significant.fit(X_train_significant, y_train_significant)\n",
    "\n",
    "# The coefficients\n",
    "print('Estimated betas: \\n', logit_significant.coef_)\n",
    "print('Estimated beta0: \\n', logit_significant.intercept_)\n",
    "\n",
    "train_score_significant = logit_significant.score(X_train_significant, y_train_significant)\n",
    "test_score_significant = logit_significant.score(X_test_significant, y_test_significant)\n",
    "print('Training score of model', train_score_significant)\n",
    "print('Testing score of model', test_score_significant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (e): Dimensionality Reduction using PCA\n",
    "\n",
    "A reasonable approach to reduce the dimensionality of the data is to use PCA and fit a logistic regression model on the first set of principal components contributing to 90% of the variance in the predictors.\n",
    "\n",
    "1. How do the classification accuracy values on both the training and tests sets compare with the models fitted in Parts (c) and (d)?  \n",
    "\n",
    "2. Re-fit a logistic regression model using 5-fold cross-validation to choose the number of principal components, and comment on whether you get better test performance than the model fitted above (explain your observations). \n",
    "\n",
    "3. Use the code provided in Part (c) to visualize the probabilities predicted by the fitted models on both the training and test sets. How does the spread of probabilities in these plots compare to those for the models in Part (c) and (d)? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Our best model used 24 PCA components with a testing accuracy of almost 93%. \n",
    "This matches the accuracy of the above model with all predictors, but has the advantage of being a simpler model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the number of PCA components whose explained variance is > 90%\n",
    "for i in range(0, len(X_train[1])):\n",
    "    pca = PCA(n_components=i)\n",
    "    pca.fit(X_train)\n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    if pca.explained_variance_ratio_.sum() > .9:\n",
    "        components = i\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated betas: \n",
      " [[ 0.02556793 -0.0393443   0.01185487 ...,  0.00151029  0.09575496\n",
      "   0.00958508]]\n",
      "Estimated beta0: \n",
      " [-0.00831347]\n",
      "\n",
      "\n",
      "Training score of model 1.0\n",
      "Testing score of model 0.926829268293\n",
      "\n",
      "\n",
      "Number of PCA components used 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wlt42\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Create logistic regression object\n",
    "logit_pca = LogisticRegression(C = 1000000)\n",
    "logit_pca.fit (X_train_pca, y_train)\n",
    "\n",
    "# The coefficients\n",
    "print('Estimated betas: \\n', logitm.coef_)\n",
    "print('Estimated beta0: \\n', logitm.intercept_)\n",
    "\n",
    "train_score = logitm.score(X_train, y_train)\n",
    "test_score = logitm.score(X_test, y_test)\n",
    "print('\\n')\n",
    "print('Training score of model', train_score)\n",
    "print('Testing score of model', test_score)\n",
    "print('\\n')\n",
    "print('Number of PCA components used', components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 7129) (32,)\n",
      "(41, 7129) (41,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_train = y_train.reshape(len(y_train),)\n",
    "y_test = y_test.reshape(len(y_test),)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wlt42\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\wlt42\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\pca.py:398: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters of the model are: {'logistic__C': 1000000, 'pca__n_components': 18}\n",
      "With a mean testing score of: 0.90625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_logistic__C</th>\n",
       "      <th>param_pca__n_components</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.024974</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.65625</td>\n",
       "      <td>0.608947</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 1}</td>\n",
       "      <td>1995</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.003531</td>\n",
       "      <td>9.508329e-05</td>\n",
       "      <td>0.029496</td>\n",
       "      <td>0.027455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.032082</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.59375</td>\n",
       "      <td>0.720779</td>\n",
       "      <td>1000000</td>\n",
       "      <td>2</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 2}</td>\n",
       "      <td>1997</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>1.893236e-03</td>\n",
       "      <td>0.037924</td>\n",
       "      <td>0.093867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020402</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.71875</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>1000000</td>\n",
       "      <td>3</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 3}</td>\n",
       "      <td>1988</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003395</td>\n",
       "      <td>7.792827e-04</td>\n",
       "      <td>0.066492</td>\n",
       "      <td>0.080937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.029016</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>0.71875</td>\n",
       "      <td>0.968254</td>\n",
       "      <td>1000000</td>\n",
       "      <td>4</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 4}</td>\n",
       "      <td>1988</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004121</td>\n",
       "      <td>1.701032e-03</td>\n",
       "      <td>0.088489</td>\n",
       "      <td>0.044896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.029183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.71875</td>\n",
       "      <td>0.968254</td>\n",
       "      <td>1000000</td>\n",
       "      <td>5</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 5}</td>\n",
       "      <td>1988</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008733</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.088489</td>\n",
       "      <td>0.044896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.031928</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>0.71875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>6</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 6}</td>\n",
       "      <td>1988</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>1.412819e-03</td>\n",
       "      <td>0.088489</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.031936</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>0.75000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>7</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 7}</td>\n",
       "      <td>1985</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001849</td>\n",
       "      <td>6.432164e-04</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.033771</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.81250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>8</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 8}</td>\n",
       "      <td>1973</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002417</td>\n",
       "      <td>2.104870e-03</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.039174</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.81250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>9</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 9}</td>\n",
       "      <td>1973</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002720</td>\n",
       "      <td>1.412877e-03</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.036612</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.81250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>10</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 10}</td>\n",
       "      <td>1973</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>2.533883e-04</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.040107</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.81250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>11</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 11}</td>\n",
       "      <td>1973</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002490</td>\n",
       "      <td>1.857014e-06</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.040739</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>12</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 12}</td>\n",
       "      <td>1981</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>1.344807e-05</td>\n",
       "      <td>0.039752</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.036146</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>13</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 13}</td>\n",
       "      <td>1981</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003973</td>\n",
       "      <td>4.115845e-04</td>\n",
       "      <td>0.039752</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.028910</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>14</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 14}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>7.084042e-04</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.027725</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>15</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 15}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004367</td>\n",
       "      <td>2.364162e-04</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.036110</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>16</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 16}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009114</td>\n",
       "      <td>1.815828e-03</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.020518</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>17</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 17}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010483</td>\n",
       "      <td>5.611062e-04</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.012288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>18</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 18}</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.080943</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.011850</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>19</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 19}</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>1.003910e-03</td>\n",
       "      <td>0.090767</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.017128</td>\n",
       "      <td>0.003283</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>20</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 20}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>1.404202e-03</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.016650</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>21</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 21}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000857</td>\n",
       "      <td>1.752740e-03</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.015933</td>\n",
       "      <td>0.001424</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>22</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 22}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002534</td>\n",
       "      <td>2.013608e-03</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.016612</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>23</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 23}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>1.413550e-03</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.014367</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>24</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 24}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003798</td>\n",
       "      <td>4.884559e-04</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.017171</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>25</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 25}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002331</td>\n",
       "      <td>1.754857e-03</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.012306</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>26</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 26}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001934</td>\n",
       "      <td>4.731124e-04</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.013878</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>27</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 27}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003825</td>\n",
       "      <td>6.928949e-05</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.013109</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>28</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 28}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002546</td>\n",
       "      <td>2.247832e-07</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.009815</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>29</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 29}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>1.072147e-06</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.010121</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>30</td>\n",
       "      <td>{'logistic__C': 1000000, 'pca__n_components': 30}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>8.485379e-07</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>0.014949</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>970</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002652</td>\n",
       "      <td>4.728314e-04</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>0.019070</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>971</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002027</td>\n",
       "      <td>4.748545e-04</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>0.019187</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>972</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>4.728314e-04</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>0.016672</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>973</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>1.948870e-03</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>0.016783</td>\n",
       "      <td>0.002698</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>974</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>1.907803e-03</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>0.016890</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>975</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>1.886493e-03</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>0.017656</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>976</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001699</td>\n",
       "      <td>4.732810e-04</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>0.016723</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>977</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001781</td>\n",
       "      <td>4.723819e-04</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>0.017078</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>978</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>1.886156e-03</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>0.016711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>979</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>0.017815</td>\n",
       "      <td>0.002709</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>980</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>1.915845e-03</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>0.016762</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>981</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>1.945499e-03</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>0.014735</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>982</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005422</td>\n",
       "      <td>4.714362e-04</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>0.017871</td>\n",
       "      <td>0.002813</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>983</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>1.996181e-03</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>0.014503</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>984</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002076</td>\n",
       "      <td>4.723819e-04</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>0.018210</td>\n",
       "      <td>0.002015</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>985</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>1.649631e-03</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>0.020555</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>986</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>5.947204e-07</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>0.019780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>987</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>0.016904</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>988</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>0.014917</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>989</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002231</td>\n",
       "      <td>1.414336e-03</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>0.017689</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>990</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002852</td>\n",
       "      <td>7.985521e-04</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>0.018138</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>991</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>1.706155e-03</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>0.017882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>992</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001914</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>0.017171</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>993</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1.886381e-03</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>0.017218</td>\n",
       "      <td>0.003107</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>994</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>2.236763e-03</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>0.019889</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>995</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002207</td>\n",
       "      <td>9.453257e-04</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>0.012283</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>996</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004163</td>\n",
       "      <td>1.751647e-03</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.013019</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>997</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>1.474175e-03</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>998</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>1.286178e-04</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.014718</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1000001</td>\n",
       "      <td>999</td>\n",
       "      <td>{'logistic__C': 1000001, 'pca__n_components': ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>3.104256e-04</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1998 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0          0.024974         0.000067          0.65625          0.608947   \n",
       "1          0.032082         0.001339          0.59375          0.720779   \n",
       "2          0.020402         0.001081          0.71875          0.920635   \n",
       "3          0.029016         0.001670          0.71875          0.968254   \n",
       "4          0.029183         0.000000          0.71875          0.968254   \n",
       "5          0.031928         0.003001          0.71875          1.000000   \n",
       "6          0.031936         0.000616          0.75000          1.000000   \n",
       "7          0.033771         0.001488          0.81250          1.000000   \n",
       "8          0.039174         0.003003          0.81250          1.000000   \n",
       "9          0.036612         0.001151          0.81250          1.000000   \n",
       "10         0.040107         0.001003          0.81250          1.000000   \n",
       "11         0.040739         0.001013          0.78125          1.000000   \n",
       "12         0.036146         0.000973          0.78125          1.000000   \n",
       "13         0.028910         0.000501          0.87500          1.000000   \n",
       "14         0.027725         0.000836          0.87500          1.000000   \n",
       "15         0.036110         0.001752          0.87500          1.000000   \n",
       "16         0.020518         0.000773          0.87500          1.000000   \n",
       "17         0.012288         0.000000          0.90625          1.000000   \n",
       "18         0.011850         0.002587          0.87500          1.000000   \n",
       "19         0.017128         0.003283          0.87500          1.000000   \n",
       "20         0.016650         0.001706          0.87500          1.000000   \n",
       "21         0.015933         0.001424          0.87500          1.000000   \n",
       "22         0.016612         0.002003          0.87500          1.000000   \n",
       "23         0.014367         0.000690          0.87500          1.000000   \n",
       "24         0.017171         0.001709          0.87500          1.000000   \n",
       "25         0.012306         0.000669          0.87500          1.000000   \n",
       "26         0.013878         0.001052          0.87500          1.000000   \n",
       "27         0.013109         0.001003          0.87500          1.000000   \n",
       "28         0.009815         0.001004          0.87500          1.000000   \n",
       "29         0.010121         0.001003          0.87500          1.000000   \n",
       "...             ...              ...              ...               ...   \n",
       "1968       0.014949         0.000334          0.87500          1.000000   \n",
       "1969       0.019070         0.000336          0.87500          1.000000   \n",
       "1970       0.019187         0.000334          0.87500          1.000000   \n",
       "1971       0.016672         0.001378          0.87500          1.000000   \n",
       "1972       0.016783         0.002698          0.87500          1.000000   \n",
       "1973       0.016890         0.001334          0.87500          1.000000   \n",
       "1974       0.017656         0.000335          0.87500          1.000000   \n",
       "1975       0.016723         0.000334          0.87500          1.000000   \n",
       "1976       0.017078         0.002667          0.87500          1.000000   \n",
       "1977       0.016711         0.000000          0.87500          1.000000   \n",
       "1978       0.017815         0.002709          0.87500          1.000000   \n",
       "1979       0.016762         0.001376          0.87500          1.000000   \n",
       "1980       0.014735         0.000667          0.87500          1.000000   \n",
       "1981       0.017871         0.002813          0.87500          1.000000   \n",
       "1982       0.014503         0.000334          0.87500          1.000000   \n",
       "1983       0.018210         0.002015          0.87500          1.000000   \n",
       "1984       0.020555         0.001003          0.87500          1.000000   \n",
       "1985       0.019780         0.000000          0.87500          1.000000   \n",
       "1986       0.016904         0.000000          0.87500          1.000000   \n",
       "1987       0.014917         0.002003          0.87500          1.000000   \n",
       "1988       0.017689         0.002988          0.87500          1.000000   \n",
       "1989       0.018138         0.002349          0.87500          1.000000   \n",
       "1990       0.017882         0.000000          0.87500          1.000000   \n",
       "1991       0.017171         0.001334          0.87500          1.000000   \n",
       "1992       0.017218         0.003107          0.87500          1.000000   \n",
       "1993       0.019889         0.000668          0.87500          1.000000   \n",
       "1994       0.012283         0.001705          0.87500          1.000000   \n",
       "1995       0.013019         0.001501          0.87500          1.000000   \n",
       "1996       0.013245         0.000881          0.87500          1.000000   \n",
       "1997       0.014718         0.000220          0.87500          1.000000   \n",
       "\n",
       "     param_logistic__C param_pca__n_components  \\\n",
       "0              1000000                       1   \n",
       "1              1000000                       2   \n",
       "2              1000000                       3   \n",
       "3              1000000                       4   \n",
       "4              1000000                       5   \n",
       "5              1000000                       6   \n",
       "6              1000000                       7   \n",
       "7              1000000                       8   \n",
       "8              1000000                       9   \n",
       "9              1000000                      10   \n",
       "10             1000000                      11   \n",
       "11             1000000                      12   \n",
       "12             1000000                      13   \n",
       "13             1000000                      14   \n",
       "14             1000000                      15   \n",
       "15             1000000                      16   \n",
       "16             1000000                      17   \n",
       "17             1000000                      18   \n",
       "18             1000000                      19   \n",
       "19             1000000                      20   \n",
       "20             1000000                      21   \n",
       "21             1000000                      22   \n",
       "22             1000000                      23   \n",
       "23             1000000                      24   \n",
       "24             1000000                      25   \n",
       "25             1000000                      26   \n",
       "26             1000000                      27   \n",
       "27             1000000                      28   \n",
       "28             1000000                      29   \n",
       "29             1000000                      30   \n",
       "...                ...                     ...   \n",
       "1968           1000001                     970   \n",
       "1969           1000001                     971   \n",
       "1970           1000001                     972   \n",
       "1971           1000001                     973   \n",
       "1972           1000001                     974   \n",
       "1973           1000001                     975   \n",
       "1974           1000001                     976   \n",
       "1975           1000001                     977   \n",
       "1976           1000001                     978   \n",
       "1977           1000001                     979   \n",
       "1978           1000001                     980   \n",
       "1979           1000001                     981   \n",
       "1980           1000001                     982   \n",
       "1981           1000001                     983   \n",
       "1982           1000001                     984   \n",
       "1983           1000001                     985   \n",
       "1984           1000001                     986   \n",
       "1985           1000001                     987   \n",
       "1986           1000001                     988   \n",
       "1987           1000001                     989   \n",
       "1988           1000001                     990   \n",
       "1989           1000001                     991   \n",
       "1990           1000001                     992   \n",
       "1991           1000001                     993   \n",
       "1992           1000001                     994   \n",
       "1993           1000001                     995   \n",
       "1994           1000001                     996   \n",
       "1995           1000001                     997   \n",
       "1996           1000001                     998   \n",
       "1997           1000001                     999   \n",
       "\n",
       "                                                 params  rank_test_score  \\\n",
       "0      {'logistic__C': 1000000, 'pca__n_components': 1}             1995   \n",
       "1      {'logistic__C': 1000000, 'pca__n_components': 2}             1997   \n",
       "2      {'logistic__C': 1000000, 'pca__n_components': 3}             1988   \n",
       "3      {'logistic__C': 1000000, 'pca__n_components': 4}             1988   \n",
       "4      {'logistic__C': 1000000, 'pca__n_components': 5}             1988   \n",
       "5      {'logistic__C': 1000000, 'pca__n_components': 6}             1988   \n",
       "6      {'logistic__C': 1000000, 'pca__n_components': 7}             1985   \n",
       "7      {'logistic__C': 1000000, 'pca__n_components': 8}             1973   \n",
       "8      {'logistic__C': 1000000, 'pca__n_components': 9}             1973   \n",
       "9     {'logistic__C': 1000000, 'pca__n_components': 10}             1973   \n",
       "10    {'logistic__C': 1000000, 'pca__n_components': 11}             1973   \n",
       "11    {'logistic__C': 1000000, 'pca__n_components': 12}             1981   \n",
       "12    {'logistic__C': 1000000, 'pca__n_components': 13}             1981   \n",
       "13    {'logistic__C': 1000000, 'pca__n_components': 14}                3   \n",
       "14    {'logistic__C': 1000000, 'pca__n_components': 15}                3   \n",
       "15    {'logistic__C': 1000000, 'pca__n_components': 16}                3   \n",
       "16    {'logistic__C': 1000000, 'pca__n_components': 17}                3   \n",
       "17    {'logistic__C': 1000000, 'pca__n_components': 18}                1   \n",
       "18    {'logistic__C': 1000000, 'pca__n_components': 19}                3   \n",
       "19    {'logistic__C': 1000000, 'pca__n_components': 20}                3   \n",
       "20    {'logistic__C': 1000000, 'pca__n_components': 21}                3   \n",
       "21    {'logistic__C': 1000000, 'pca__n_components': 22}                3   \n",
       "22    {'logistic__C': 1000000, 'pca__n_components': 23}                3   \n",
       "23    {'logistic__C': 1000000, 'pca__n_components': 24}                3   \n",
       "24    {'logistic__C': 1000000, 'pca__n_components': 25}                3   \n",
       "25    {'logistic__C': 1000000, 'pca__n_components': 26}                3   \n",
       "26    {'logistic__C': 1000000, 'pca__n_components': 27}                3   \n",
       "27    {'logistic__C': 1000000, 'pca__n_components': 28}                3   \n",
       "28    {'logistic__C': 1000000, 'pca__n_components': 29}                3   \n",
       "29    {'logistic__C': 1000000, 'pca__n_components': 30}                3   \n",
       "...                                                 ...              ...   \n",
       "1968  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1969  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1970  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1971  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1972  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1973  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1974  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1975  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1976  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1977  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1978  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1979  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1980  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1981  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1982  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1983  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1984  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1985  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1986  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1987  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1988  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1989  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1990  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1991  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1992  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1993  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1994  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1995  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1996  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "1997  {'logistic__C': 1000001, 'pca__n_components': ...                3   \n",
       "\n",
       "      split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0              0.636364            0.619048           0.636364   \n",
       "1              0.636364            0.809524           0.545455   \n",
       "2              0.727273            0.952381           0.636364   \n",
       "3              0.727273            0.904762           0.818182   \n",
       "4              0.727273            0.904762           0.818182   \n",
       "5              0.818182            1.000000           0.727273   \n",
       "6              0.818182            1.000000           0.727273   \n",
       "7              0.818182            1.000000           0.818182   \n",
       "8              0.818182            1.000000           0.818182   \n",
       "9              0.818182            1.000000           0.818182   \n",
       "10             0.818182            1.000000           0.818182   \n",
       "11             0.727273            1.000000           0.818182   \n",
       "12             0.727273            1.000000           0.818182   \n",
       "13             0.909091            1.000000           0.909091   \n",
       "14             0.909091            1.000000           0.909091   \n",
       "15             0.909091            1.000000           0.909091   \n",
       "16             0.909091            1.000000           0.909091   \n",
       "17             1.000000            1.000000           0.909091   \n",
       "18             1.000000            1.000000           0.818182   \n",
       "19             0.909091            1.000000           0.909091   \n",
       "20             0.909091            1.000000           0.909091   \n",
       "21             0.909091            1.000000           0.909091   \n",
       "22             0.909091            1.000000           0.909091   \n",
       "23             0.909091            1.000000           0.909091   \n",
       "24             0.909091            1.000000           0.909091   \n",
       "25             0.909091            1.000000           0.909091   \n",
       "26             0.909091            1.000000           0.909091   \n",
       "27             0.909091            1.000000           0.909091   \n",
       "28             0.909091            1.000000           0.909091   \n",
       "29             0.909091            1.000000           0.909091   \n",
       "...                 ...                 ...                ...   \n",
       "1968           0.909091            1.000000           0.909091   \n",
       "1969           0.909091            1.000000           0.909091   \n",
       "1970           0.909091            1.000000           0.909091   \n",
       "1971           0.909091            1.000000           0.909091   \n",
       "1972           0.909091            1.000000           0.909091   \n",
       "1973           0.909091            1.000000           0.909091   \n",
       "1974           0.909091            1.000000           0.909091   \n",
       "1975           0.909091            1.000000           0.909091   \n",
       "1976           0.909091            1.000000           0.909091   \n",
       "1977           0.909091            1.000000           0.909091   \n",
       "1978           0.909091            1.000000           0.909091   \n",
       "1979           0.909091            1.000000           0.909091   \n",
       "1980           0.909091            1.000000           0.909091   \n",
       "1981           0.909091            1.000000           0.909091   \n",
       "1982           0.909091            1.000000           0.909091   \n",
       "1983           0.909091            1.000000           0.909091   \n",
       "1984           0.909091            1.000000           0.909091   \n",
       "1985           0.909091            1.000000           0.909091   \n",
       "1986           0.909091            1.000000           0.909091   \n",
       "1987           0.909091            1.000000           0.909091   \n",
       "1988           0.909091            1.000000           0.909091   \n",
       "1989           0.909091            1.000000           0.909091   \n",
       "1990           0.909091            1.000000           0.909091   \n",
       "1991           0.909091            1.000000           0.909091   \n",
       "1992           0.909091            1.000000           0.909091   \n",
       "1993           0.909091            1.000000           0.909091   \n",
       "1994           0.909091            1.000000           0.909091   \n",
       "1995           0.909091            1.000000           0.909091   \n",
       "1996           0.909091            1.000000           0.909091   \n",
       "1997           0.909091            1.000000           0.909091   \n",
       "\n",
       "      split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0               0.571429                0.7            0.636364      0.003531   \n",
       "1               0.761905                0.6            0.590909      0.000877   \n",
       "2               0.809524                0.8            1.000000      0.003395   \n",
       "3               1.000000                0.6            1.000000      0.004121   \n",
       "4               1.000000                0.6            1.000000      0.008733   \n",
       "5               1.000000                0.6            1.000000      0.001755   \n",
       "6               1.000000                0.7            1.000000      0.001849   \n",
       "7               1.000000                0.8            1.000000      0.002417   \n",
       "8               1.000000                0.8            1.000000      0.002720   \n",
       "9               1.000000                0.8            1.000000      0.000646   \n",
       "10              1.000000                0.8            1.000000      0.002490   \n",
       "11              1.000000                0.8            1.000000      0.001677   \n",
       "12              1.000000                0.8            1.000000      0.003973   \n",
       "13              1.000000                0.8            1.000000      0.003950   \n",
       "14              1.000000                0.8            1.000000      0.004367   \n",
       "15              1.000000                0.8            1.000000      0.009114   \n",
       "16              1.000000                0.8            1.000000      0.010483   \n",
       "17              1.000000                0.8            1.000000      0.001913   \n",
       "18              1.000000                0.8            1.000000      0.001647   \n",
       "19              1.000000                0.8            1.000000      0.004345   \n",
       "20              1.000000                0.8            1.000000      0.000857   \n",
       "21              1.000000                0.8            1.000000      0.002534   \n",
       "22              1.000000                0.8            1.000000      0.001346   \n",
       "23              1.000000                0.8            1.000000      0.003798   \n",
       "24              1.000000                0.8            1.000000      0.002331   \n",
       "25              1.000000                0.8            1.000000      0.001934   \n",
       "26              1.000000                0.8            1.000000      0.003825   \n",
       "27              1.000000                0.8            1.000000      0.002546   \n",
       "28              1.000000                0.8            1.000000      0.000743   \n",
       "29              1.000000                0.8            1.000000      0.001317   \n",
       "...                  ...                ...                 ...           ...   \n",
       "1968            1.000000                0.8            1.000000      0.002652   \n",
       "1969            1.000000                0.8            1.000000      0.002027   \n",
       "1970            1.000000                0.8            1.000000      0.000846   \n",
       "1971            1.000000                0.8            1.000000      0.001690   \n",
       "1972            1.000000                0.8            1.000000      0.000554   \n",
       "1973            1.000000                0.8            1.000000      0.001223   \n",
       "1974            1.000000                0.8            1.000000      0.001699   \n",
       "1975            1.000000                0.8            1.000000      0.001781   \n",
       "1976            1.000000                0.8            1.000000      0.002086   \n",
       "1977            1.000000                0.8            1.000000      0.000504   \n",
       "1978            1.000000                0.8            1.000000      0.003240   \n",
       "1979            1.000000                0.8            1.000000      0.000527   \n",
       "1980            1.000000                0.8            1.000000      0.005422   \n",
       "1981            1.000000                0.8            1.000000      0.000962   \n",
       "1982            1.000000                0.8            1.000000      0.002076   \n",
       "1983            1.000000                0.8            1.000000      0.000994   \n",
       "1984            1.000000                0.8            1.000000      0.001223   \n",
       "1985            1.000000                0.8            1.000000      0.001299   \n",
       "1986            1.000000                0.8            1.000000      0.003814   \n",
       "1987            1.000000                0.8            1.000000      0.002231   \n",
       "1988            1.000000                0.8            1.000000      0.002852   \n",
       "1989            1.000000                0.8            1.000000      0.001334   \n",
       "1990            1.000000                0.8            1.000000      0.001914   \n",
       "1991            1.000000                0.8            1.000000      0.000019   \n",
       "1992            1.000000                0.8            1.000000      0.000951   \n",
       "1993            1.000000                0.8            1.000000      0.002207   \n",
       "1994            1.000000                0.8            1.000000      0.004163   \n",
       "1995            1.000000                0.8            1.000000      0.001763   \n",
       "1996            1.000000                0.8            1.000000      0.001441   \n",
       "1997            1.000000                0.8            1.000000      0.000153   \n",
       "\n",
       "      std_score_time  std_test_score  std_train_score  \n",
       "0       9.508329e-05        0.029496         0.027455  \n",
       "1       1.893236e-03        0.037924         0.093867  \n",
       "2       7.792827e-04        0.066492         0.080937  \n",
       "3       1.701032e-03        0.088489         0.044896  \n",
       "4       0.000000e+00        0.088489         0.044896  \n",
       "5       1.412819e-03        0.088489         0.000000  \n",
       "6       6.432164e-04        0.050565         0.000000  \n",
       "7       2.104870e-03        0.008427         0.000000  \n",
       "8       1.412877e-03        0.008427         0.000000  \n",
       "9       2.533883e-04        0.008427         0.000000  \n",
       "10      1.857014e-06        0.008427         0.000000  \n",
       "11      1.344807e-05        0.039752         0.000000  \n",
       "12      4.115845e-04        0.039752         0.000000  \n",
       "13      7.084042e-04        0.050565         0.000000  \n",
       "14      2.364162e-04        0.050565         0.000000  \n",
       "15      1.815828e-03        0.050565         0.000000  \n",
       "16      5.611062e-04        0.050565         0.000000  \n",
       "17      0.000000e+00        0.080943         0.000000  \n",
       "18      1.003910e-03        0.090767         0.000000  \n",
       "19      1.404202e-03        0.050565         0.000000  \n",
       "20      1.752740e-03        0.050565         0.000000  \n",
       "21      2.013608e-03        0.050565         0.000000  \n",
       "22      1.413550e-03        0.050565         0.000000  \n",
       "23      4.884559e-04        0.050565         0.000000  \n",
       "24      1.754857e-03        0.050565         0.000000  \n",
       "25      4.731124e-04        0.050565         0.000000  \n",
       "26      6.928949e-05        0.050565         0.000000  \n",
       "27      2.247832e-07        0.050565         0.000000  \n",
       "28      1.072147e-06        0.050565         0.000000  \n",
       "29      8.485379e-07        0.050565         0.000000  \n",
       "...              ...             ...              ...  \n",
       "1968    4.728314e-04        0.050565         0.000000  \n",
       "1969    4.748545e-04        0.050565         0.000000  \n",
       "1970    4.728314e-04        0.050565         0.000000  \n",
       "1971    1.948870e-03        0.050565         0.000000  \n",
       "1972    1.907803e-03        0.050565         0.000000  \n",
       "1973    1.886493e-03        0.050565         0.000000  \n",
       "1974    4.732810e-04        0.050565         0.000000  \n",
       "1975    4.723819e-04        0.050565         0.000000  \n",
       "1976    1.886156e-03        0.050565         0.000000  \n",
       "1977    0.000000e+00        0.050565         0.000000  \n",
       "1978    1.915845e-03        0.050565         0.000000  \n",
       "1979    1.945499e-03        0.050565         0.000000  \n",
       "1980    4.714362e-04        0.050565         0.000000  \n",
       "1981    1.996181e-03        0.050565         0.000000  \n",
       "1982    4.723819e-04        0.050565         0.000000  \n",
       "1983    1.649631e-03        0.050565         0.000000  \n",
       "1984    5.947204e-07        0.050565         0.000000  \n",
       "1985    0.000000e+00        0.050565         0.000000  \n",
       "1986    0.000000e+00        0.050565         0.000000  \n",
       "1987    1.414336e-03        0.050565         0.000000  \n",
       "1988    7.985521e-04        0.050565         0.000000  \n",
       "1989    1.706155e-03        0.050565         0.000000  \n",
       "1990    0.000000e+00        0.050565         0.000000  \n",
       "1991    1.886381e-03        0.050565         0.000000  \n",
       "1992    2.236763e-03        0.050565         0.000000  \n",
       "1993    9.453257e-04        0.050565         0.000000  \n",
       "1994    1.751647e-03        0.050565         0.000000  \n",
       "1995    1.474175e-03        0.050565         0.000000  \n",
       "1996    1.286178e-04        0.050565         0.000000  \n",
       "1997    3.104256e-04        0.050565         0.000000  \n",
       "\n",
       "[1998 rows x 18 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_pipe = LogisticRegression()\n",
    "pca_pipe = decomposition.PCA()\n",
    "pca_pipe.fit(X_train[1])\n",
    "\n",
    "pipe = Pipeline(steps=[('pca', pca_pipe), ('logistic', clf_pipe)])\n",
    "#n_components = [32, 64, 128, 256, 512, 768, 1024, 1200]\n",
    "n_components = list(np.arange(1, 1000, 1))\n",
    "estimator = GridSearchCV(pipe, dict(pca__n_components=n_components, logistic__C=(1000000, 1000001)))\n",
    "estimator.fit(X_train, y_train)\n",
    "estimator.cv_results_.keys()\n",
    "print('The best parameters of the model are:', estimator.best_params_)\n",
    "#\n",
    "#print('With a mean training score of:', estimator.mean_train_score )\n",
    "print('With a mean testing score of:', estimator.best_score_)\n",
    "\n",
    "results_df = pd.DataFrame(estimator.cv_results_)\n",
    "#estimator.cv_results_()\n",
    "# The coefficv_results_()cients\n",
    "#print('Estimated betas: \\n', estimated.coef_)\n",
    "#print('Estimated beta0: \\n', estimated.intercept_)\n",
    "\n",
    "#train_score_pipe = estimated.score(X_train, y_train)\n",
    "#test_score_pipe = estimated.score(X_test, y_test)\n",
    "#print('\\n')\n",
    "#print('Training score of model', train_score_pipe)\n",
    "#print('Testing score of model', test_score_pipe)\n",
    "#print('\\n')\n",
    "#print('Number of PCA components used', n_components)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff7682ca2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD0CAYAAACIPxFSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMMYYExZLGMYYY8Ly/wGUynt9IaxG7gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff7682ca710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax2 = fig.add_subplot(121)\n",
    "#plt.axes([.2, .2, .7, .7])\n",
    "ax2 = plt.subplot(2, 1, 1)\n",
    "ax2.plot(pca.explained_variance_, linewidth=2)\n",
    "plt.axis('tight')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('explained_variance')\n",
    "\n",
    "ax2.axvline(estimator.best_estimator_.named_steps['pca'].n_components,\n",
    "            linestyle=':', label='n_components chosen')\n",
    "ax2.legend(prop=dict(size=12))\n",
    "\n",
    "ax1 = fig.add_subplot(122)\n",
    "ax1 = plt.subplot(2, 1, 2)\n",
    "y_train_viz = data_train[\"Cancer_type\"]\n",
    "visualize_prob(estimator.best_estimator_, X_train, y_train_viz, ax1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APCOMP209a - Homework Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to conduct PCA on the model matrix $X \\in \\Re^{n×p}$, where the columns have been suitably set to zero mean. In this question, we consider the squared reconstruction error:\n",
    "\n",
    "$$  \\parallel XQ- XQ_m \\parallel ^2 $$\n",
    "\n",
    "for a suitable set of eigenvectors forming the matrix $Q_m$, as discussed below. Suppose that we conduct eigendecomposition of $X^T X$ and obtain eigenvalues $\\lambda_1, \\ldots , \\lambda_p$ and principal components $Q$, i.e.\n",
    "\n",
    "$$ X^T X = Q \\Lambda Q ^T $$\n",
    "\n",
    "(1) Suppose that the matrix norm is simply the squared dot product, namely\n",
    "\n",
    "$$ \\parallel A \\parallel ^2 = A^T A $$\n",
    "\n",
    "Then, express the reconstruction error as a sum of matrix products.\n",
    "\n",
    "(2) Simplify your result from (1) based on properties of the matrices $Q$.\n",
    "\n",
    "(3) Now let $Q_m$ be the matrix of the first $m < p$ eigenvectors, namely\n",
    "\n",
    "$$ Q_m = (q_1, \\ldots, q_m, 0, \\ldots, 0) \\in \\Re^{p \\times p} $$\n",
    "\n",
    "Thus, $X Q_m$ is the PCA projection of the data into the space spanned by the first $m$ principal components. Express the products $Q^T_m Q$ and $Q^T Q_m$, again using properties of the eigenbasis $q_1, \\ldots, q_p$.\n",
    "\n",
    "(4) Use your results from (3) to finally fully simplify your expression from (2).\n",
    "\n",
    "(5) Note that the result you obtain should still be a matrix, i.e. this does not define a proper norm on the space of matrices (since the value should be a scalar). Consequently, the true matrix norm is actually the trace of the\n",
    "above result, namely\n",
    "\n",
    "$$ \\parallel A \\parallel ^2  = {\\rm trace} (A^T A) $$\n",
    "Use your result from (4) and this new definition to find a simple expression\n",
    "for the reconstruction error in terms of the eigenvalues.\n",
    "\n",
    "(6) Interpret your result from (5). In light of your results, does our procedure for PCA (selecting the $m$ substantially larger eigenvalues) make sense? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
